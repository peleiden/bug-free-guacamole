@inproceedings{yamada2020luke,
    title = "{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention",
    author = "Yamada, Ikuya  and
      Asai, Akari  and
      Shindo, Hiroyuki  and
      Takeda, Hideaki  and
      Matsumoto, Yuji",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.523",
    doi = "10.18653/v1/2020.emnlp-main.523",
    pages = "6442--6454"
}

@inproceedings{hvingelby2020dane,
    title = "{D}a{NE}: A Named Entity Resource for {D}anish",
    author = "Hvingelby, Rasmus  and
      Pauli, Amalie Brogaard  and
      Barrett, Maria  and
      Rosted, Christina  and
      Lidegaard, Lasse Malm  and
      S{\o}gaard, Anders",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.565",
    pages = "4597--4604",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{derc2014dkie,
    title = "{DKIE}: Open Source Information Extraction for {D}anish",
    author = "Derczynski, Leon  and
      Field, Camilla Vilhelmsen  and
      B{\o}gh, Kenneth S.",
    booktitle = "Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E14-2016",
    doi = "10.3115/v1/E14-2016",
    pages = "61--64",
}

@misc{derc2019simple,
      title={Simple Natural Language Processing Tools for Danish}, 
      author={Leon Derczynski},
      year={2019},
      eprint={1906.11608},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{johann2015udddt,
    author = {Anders Johannsen and Hector Martinez and Alonso Barbara Plank},
    title = {Universal Dependencies for {D}anish},
    year = {2015},
    booktitle = {TLT14},
}

@misc{christensen1998parole,
    author = {Ole Norling-Christensen and Britt-Katrin Keson and J{\o}rg Asmussen},
    title = {PAROLE-DK and ePAROLE: Morphosyntactically Annotated Danish Language Corpus},
    year = {1998},
    publisher = {The Society for Danish Language and Literature, DSL},
}
@inproceedings{plank2019neural,
    title = "Neural Cross-Lingual Transfer and Limited Annotated Data for Named Entity Recognition in {D}anish",
    author = "Plank, Barbara",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://www.aclweb.org/anthology/W19-6143",
    pages = "370--375",
    abstract = "Named Entity Recognition (NER) has greatly advanced by the introduction of deep neural architectures. However, the success of these methods depends on large amounts of training data. The scarcity of publicly-available human-labeled datasets has resulted in limited evaluation of existing NER systems, as is the case for Danish. This paper studies the effectiveness of cross-lingual transfer for Danish, evaluates its complementarity to limited gold data, and sheds light on performance of Danish NER.",
}

@inproceedings{pan2017wikiann,
    title = "Cross-lingual Name Tagging and Linking for 282 Languages",
    author = "Pan, Xiaoman  and
      Zhang, Boliang  and
      May, Jonathan  and
      Nothman, Joel  and
      Knight, Kevin  and
      Ji, Heng",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1178",
    doi = "10.18653/v1/P17-1178",
    pages = "1946--1958",
    abstract = "The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.",
}

@inproceedings{tjang2003conll,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://www.aclweb.org/anthology/W03-0419",
    pages = "142--147",
}


@inproceedings{rahimi2019transfer,
    title = "Massively Multilingual Transfer for {NER}",
    author = "Rahimi, Afshin  and
      Li, Yuan  and
      Cohn, Trevor",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1015",
    pages = "151--164",
}

@article{ramshaw1995IOB,
  author    = {Lance A. Ramshaw and
               Mitchell P. Marcus},
  title     = {Text Chunking using Transformation-Based Learning},
  journal   = {CoRR},
  volume    = {cmp-lg/9505040},
  year      = {1995},
  url       = {http://arxiv.org/abs/cmp-lg/9505040},
  timestamp = {Mon, 13 Aug 2018 16:48:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/cmp-lg-9505040.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@inproceedings{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@online{botxo2019dabert,
  author = {BotXO},
  title = {BotXO has trained the most advanced Danish BERT model to date},
  year = 2019,
  url = {https://www.botxo.ai/en/blog/danish-bert-model/},
  urldate = {2021-02-28}
}

@misc{clark2020electra,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@phdthesis{bertelsen2020lctra,
  author  = {Malte H{\o}jmark-Bertelsen},
  title   = {Ælæctra - A Step Towards More Efficient Danish Natural Language Processing},
  school  = {Cognitive Science, Aarhus University},
  year    = {2020},
  type    = {Bachelor's Thesis}
}

@inproceedings{akbik2019flair,
  title={FLAIR: An easy-to-use framework for state-of-the-art NLP},
  author={Akbik, Alan and Bergmann, Tanja and Blythe, Duncan and Rasul, Kashif and Schweter, Stefan and Vollgraf, Roland},
  booktitle={{NAACL} 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages={54--59},
  year={2019}
}

@article{rfou2015polyglot,
        author = {Al-Rfou, Rami and Kulkarni, Vivek and Perozzi, Bryan and Skiena, Steven},
        title = {{Polyglot-NER}: Massive Multilingual Named Entity Recognition},
        journal = {{Proceedings of the 2015 {SIAM} International Conference on Data Mining, Vancouver, British Columbia, Canada, April 30 - May 2, 2015}},
        month     = {April},
        year      = {2015},
        publisher = {SIAM}
}

@InProceedings{manning2014corenlp,
  author    = {Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David},
  title     = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
  booktitle = {Association for Computational Linguistics (ACL) System Demonstrations},
  year      = {2014},
  pages     = {55--60},
  url       = {http://www.aclweb.org/anthology/P/P14/P14-5010}
}

@INPROCEEDINGS{kromann2003ddt,
    author = {Matthias Trautner Kromann},
    title = {The Danish Dependency Treebank and the DTAG treebank tool},
    booktitle = {In Proceedings of the Second Workshop on Treebanks and Linguistic Theories (TLT 2003},
    year = {2003},
    pages = {14--15}
}

@software{honnibal2020spacy,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  title = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.1212303},
  url = {https://doi.org/10.5281/zenodo.1212303}
}

@article{vaswani2017att,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{hedderich2021survey,
    title={A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios}, 
    author={Michael A. Hedderich and Lukas Lange and Heike Adel and Jannik Strötgen and Dietrich Klakow},
    year={2021},
    eprint={2010.12309},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@book{russell2016ai,
  title={Artificial Intelligence: a modern approach},
  author={Russell, Stuart J. and Norvig, Peter},
  edition={3},
  year={2009},
  publisher={Pearson}
}

@article{otter18dlnlp,
  author    = {Daniel W. Otter and
               Julian R. Medina and
               Jugal K. Kalita},
  title     = {A Survey of the Usages of Deep Learning in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1807.10854},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.10854},
  archivePrefix = {arXiv},
  eprint    = {1807.10854},
  timestamp = {Mon, 13 Aug 2018 16:46:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-10854.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{darpa17ai,
  author = {Jon Launchbury},
  title = {DARPA Perspective on AI},
  year = 2017,
  url = {https://www.darpa.mil/about-us/darpa-perspective-on-ai},
  urldate = {2021-05-20}
}

@inproceedings{zhang2019ernie,
  title={{ERNIE}: Enhanced Language Representation with Informative Entities},
  author={Zhang, Zhengyan and Han, Xu and Liu, Zhiyuan and Jiang, Xin and Sun, Maosong and Liu, Qun},
  booktitle={Proceedings of ACL 2019},
  year={2019}
}
@inproceedings{wang2018glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}
@inproceedings{peters2019knowbert,
    title = "Knowledge Enhanced Contextual Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Logan, Robert  and
      Schwartz, Roy  and
      Joshi, Vidur  and
      Singh, Sameer  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1005",
    doi = "10.18653/v1/D19-1005",
    pages = "43--54",
    abstract = "Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert{'}s runtime is comparable to BERT{'}s and it scales to large KBs.",
}

@inproceedings{logan2019barack,
    title = "{B}arack{'}s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling",
    author = "Logan, Robert  and
      Liu, Nelson F.  and
      Peters, Matthew E.  and
      Gardner, Matt  and
      Singh, Sameer",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1598",
    doi = "10.18653/v1/P19-1598",
    pages = "5962--5971",
    abstract = "Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model{'}s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts.",
}
@mastersthesis{birk2020knowledge,
    title={Investigating state-of-the-art approaches to knowledge enhancing deep learning based language models},
    author={Victor Elkjær Birk},
    year={2020},
    school={Technical University of Denmark},
    address={Kongens Lyngby}
}

@article{wang2021kepler,
    author = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
    title = "{KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {176-194},
    year = {2021},
    month = {03},
    abstract = "{Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00360},
    url = {https://doi.org/10.1162/tacl\_a\_00360},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00360/1894315/tacl\_a\_00360.pdf},
}

@inproceedings{pytorchamp,
    author = {PyTorch Contributors},
    title = {AUTOMATIC MIXED PRECISION PACKAGE - TORCH.CUDA.AMP},
    url = {https://pytorch.org/docs/stable/amp.html},
    urldate = {2021-05-27}
}

@inproceedings{pytorchcel,
    author = {PyTorch Contributors},
    title = {CrossEntropyLoss},
    url = {https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html},
    urldate = {2021-06-09}
}

@inproceedings{huang2020amp,
    author = {Huang, Mengdi and Tekur, Chetan and Carilli, Michael},
    title = {Introducing native PyTorch automatic mixed precision for faster training on NVIDIA GPUs},
    url = {https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/},
    year = {2020},
    month = {07},
    urldate = {2021-05-28},
}

@inproceedings{mitlicense,
    author = {MIT},
    title = {The MIT License (MIT)},
    url = {https://mit-license.org/},
    urldate = {2021-05-29},
}

@inproceedings{apachelicense,
    author = {The Apache Software Foundation},
    title = {Apache License, Version 2.0},
    url = {https://www.apache.org/licenses/LICENSE-2.0.html},
    year = {2004},
    urldate = {2021-06-11},
}

@inproceedings{sohrab2018nestedner,
    author = {Mohammad Golam Sohrab and Makoto Miwa},
    title = {Deep Exhaustive Model for Nested Named Entity Recognition},
    year = {2018},
    journal = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1309.pdf},
    urldate = {2021-05-30},
    booktitle = {Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing},
    pages = {2843-2849},
}

@article{marrero2013ner,
    title = {Named Entity Recognition: Fallacies, challenges and opportunities},
    journal = {Computer Standards and Interfaces},
    volume = {35},
    number = {5},
    pages = {482-489},
    year = {2013},
    issn = {0920-5489},
    doi = {https://doi.org/10.1016/j.csi.2012.09.004},
    url = {https://www.sciencedirect.com/science/article/pii/S0920548912001080},
    author = {Mónica Marrero and Julián Urbano and Sonia Sánchez-Cuadrado and Jorge Morato and Juan Miguel Gómez-Berbís},
    keywords = {Named Entity, Named Entity Recognition, Information Extraction, Evaluation methodology, NER tools},
    abstract = {Named Entity Recognition serves as the basis for many other areas in Information Management. However, it is unclear what the meaning of Named Entity is, and yet there is a general belief that Named Entity Recognition is a solved task. In this paper we analyze the evolution of the field from a theoretical and practical point of view. We argue that the task is actually far from solved and show the consequences for the development and evaluation of tools. We discuss topics for further research with the goal of bringing the task back to the research scenario.}
}
@misc{ wiki2021ner,
author = "{Wikipedia Contributors}",
title = "Named-entity recognition --- {Wikipedia}{,} The Free Encyclopedia",
year = "2021",
howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Named-entity_recognition&oldid=1020618016}",
note = "[Online; accessed 1-June-2021]"
  }
@misc{xiong2019wklm,
      title={Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model}, 
      author={Wenhan Xiong and Jingfei Du and William Yang Wang and Veselin Stoyanov},
      year={2019},
      eprint={1912.09637},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{corby2020kalm,
  author    = {Corby Rosset and
               Chenyan Xiong and
               Minh Phan and
               Xia Song and
               Paul N. Bennett and
               Saurabh Tiwary},
  title     = {Knowledge-Aware Language Model Pretraining},
  journal   = {CoRR},
  volume    = {abs/2007.00655},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.00655},
  archivePrefix = {arXiv},
  eprint    = {2007.00655},
  timestamp = {Mon, 06 Jul 2020 15:26:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-00655.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{wang2020kadapter,
  author    = {Ruize Wang and
               Duyu Tang and
               Nan Duan and
               Zhongyu Wei and
               Xuanjing Huang and
               Jianshu Ji and
               Guihong Cao and
               Daxin Jiang and
               Ming Zhou},
  title     = {K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters},
  journal   = {CoRR},
  volume    = {abs/2002.01808},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.01808},
  archivePrefix = {arXiv},
  eprint    = {2002.01808},
  timestamp = {Wed, 12 Feb 2020 13:04:16 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-01808.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{he2020kgplm,
  author    = {Bin He and
               Xin Jiang and
               Jinghui Xiao and
               Qun Liu},
  title     = {KgPLM: Knowledge-guided Language Model Pre-training via Generative
               and Discriminative Learning},
  journal   = {CoRR},
  volume    = {abs/2012.03551},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.03551},
  archivePrefix = {arXiv},
  eprint    = {2012.03551},
  timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-03551.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{ruder21ner,
  author = {Sebastian Ruder},
  title = {NLP-progress: Named Enitty Recognition},
  year = 2021,
  url = {http://nlpprogress.com/english/named_entity_recognition.html},
  urldate = {2021-06-02}
}

@online{pwc21ner,
  author = {Papers With Code Contributors},
  title = {Papers With Code: Named Entity Recognition},
  year = 2021,
  url = {https://paperswithcode.com/task/named-entity-recognition-ner},
  urldate = {2021-06-02}
}

@inproceedings{enevoldsen2020dacy,
    title={DaCy: A SpaCy NLP Pipeline for Danish},
    author={Enevoldsen, Kenneth},
    year={2021}
}

@inproceedings{howardruder2018universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}
