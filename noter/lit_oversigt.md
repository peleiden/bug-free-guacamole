# Teori og modeller
-- Yamada et al: ["LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention"](https://arxiv.org/abs/2010.01057).
    Indfører nye, lærte representationer af ord og entiteter, som er lært ved en maskeret gætteopgave af en self-attention-mekanisme.
-- Hvingelby et al: ["DaNE: A Named Entity Resource for Danish"](https://www.aclweb.org/anthology/2020.lrec-1.565/).
    Præsenterer selve annoteringerne og træner en række modeller herunder BERT på det nye data. Fokuserer både på flersproget og rent dansk.
