
# Teori og modeller
-- Yamada et al: ["LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention"](https://arxiv.org/abs/2010.01057).
    Indfører nye, lærte representationer af ord og entiteter, som er lært ved en maskeret gætteopgave af en self-attention-mekanisme.
