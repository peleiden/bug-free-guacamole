# Foregående spørgsmål
- Hvilken rolle har "Finn's name list" og "Wikidata+Dannet" i projektplansforslaget? - De ser ikke ud til at være NER-modeller som resten (og jeg kan ikke finde dem)
- Skal vi træne BERT, spaCy, flair osv. selv? Det antager jeg ikke - vi kan vel godt bruge færdigtrænede, offentliggjorte modeller og så selv udføre testen.
- Hvilken rolle har "Finn's name list" og "Wikidata+Dannet" i projektplansforslaget?

- Hvor meget træning skal vi faktisk lave? Er det bare at hente deres ned og så køre inferens? I hvor høj grad kan vi bruges Johannes og Victors arbejde?
- Vidensgrafen/-basen: Hvordan ser den faktisk ud, og hvor kommer den fra?
- Sammenhæng mellem RoBERTa og LUKE? Starten på 3.4 er konfus
- Hvad dækker prætræning helt præcist over? De bruger prætrænet RoBERTa, men det virker mere TL-agtigt
- Forskel på prætræning og træning? Betyder træning tilpasning til hvert datasæt som antydet i bilag B?
- Hvor har de deres ordforråd fra? Det lyder til at være det samme, som RoBERTa-papiret vagt hentyder til
- Er det rigtigt forstået, at datasættet er hele wikipedia, hvor  hyperlinks <=> entiteter? Antydes i starten af 3.3. Altså at entitetsforekomster som ikke-hyperlinks tælles med, hvis >1% af forekomster er hyperlinks?
- Hvor meget træning skal vi faktisk lave på den engelske? RoBERTa er jo implementeret i torch, men den videretrænes jo også 

# Noter
- Lars Kai og Michael Riis er formelle vejledere og Victor er medvejleder. Johannes Kruse kører projekt med LUKE
- Andre relevante projekter: DanSpeech (Martin, Rasmus) har problemer, hvor LUKE ville være en god løsning
- Tekstindsamling: Danish Gigaword Project
- Data fra projekt med børnetekst kan blive relevant
- Kontrakt fra Ekstra Bladets dataafdeling Kasper Lindskov og de er igang med at lave et relevant projekt
- Fokus på udfordringerne med compute
- Flere firmaer træner BERT bl.a. alexandria
- Opgave: Forstå forskellen på BERT og RoBERTa
- Opgave: Finde ud af, hvad GPU-omkostningerne er og hvad data-omkostningerne er
- Datamuligheder: Gigaword, Gyldendal, de store korpora, børnebøger
- Der har været et treugerskursus med NER på dansk
- Johannes har reproduceret named entity typing på LUKE
- Fokus på forståelse og den bagvedliggende teknologi først
- Reproduktionen af LUKE på engelsk kan godt være begrænset til entity typing
- Lars Kai er ligeglad med reglerne for bacheloren
- Fredage klokken 10
