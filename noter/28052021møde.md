# Dagsorden
- Fortsat stor prætræning: Diskutér word/entity-tradeoff
- Eksperimenteret med entitetsbegrænsinger og entitetsvægtning: Ser ikke så lovende ud - vi vil også fine-tune
- Kører test af multi-GPU-speedup og fp16-speedup: Nævn mærkelighed med A100 for Rasmus
- Diskutere, hvor meget prætræning skal fylde i rapport og hvor mange eksperimenter, vi kan have med - se https://github.com/peleiden/daLUKE/issues/63.
- Repræsentationsgeometri - fuck perplexity! >:( "unsupervised/non-parametric" my ass
- Lossvægte indført på NER. Hurtigere konvergens; på 15 epoker med bedste lr giver en F1 på 82.5%. Diskutér, om vi kan bruge 15 epoker/lossvægtning til vores fine-tuning.
- Fokus på skrivning

# Møde
- Histogrammer af vægte kan være et godt mål
- Ændrer augmenteringen niveauet eller hastigheden?
- Kontakt Bernd med GPU'er
- Kan være en idé både at træne med BERT-like hyperparametre og med fittede hyperparametre
