# Sammenlignelige projekter
- [LUKE](https://github.com/studio-ousia/luke)
- [danlp: Afgørende samling af NLP på dansk med pretrænede NER-modeller](https://github.com/alexandrainst/danlp/)
- [ITU's entitetsarbejde på dansk](https://github.com/ITUnlp/daner)
- [Træning af dansk BERT, hvor data også beskrives godt. Anbefalet af Johannes Kruse](https://github.com/botxo/nordic_bert)
- [NERDA: Ekstra-Bladets NER-framework](https://github.com/ebanalyse/NERDA/)
- [Ælæctra: Dansk, kompakt LM testet på NER](https://github.com/MalteHB/-l-ctra)

# Data
- [Korpusset, som DKIE brugt til at annotere. Ikke direkte NER-relevant, som jeg ser det](https://github.com/mbkromann/copenhagen-dependency-treebank)
- [NE-annoteret Wikipedia](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#wikiann)
- [DaNE: Alexandra-instittets NER](https://sprogteknologi.dk/dataset/danish-dependency-treebank-dane)
- [Barbara Plancks annoterede NER](https://github.com/bplank/danish_ner_transfer)
- [DaNLP-oversigt over data](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md)

# Generelt teknisk magi
- [Anvendelse af Multi-GPU til overkommelig træning](https://github.com/yifding/hetseq)
