% !TeX spellcheck = en_GB
\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Theory and State of the Art}
\lhead{Theory and State of the Art}
\label{chap:theory}
With the advent of deep learning in the field of NLP, models have risen massively in complexity, requiring ever more compute and data.
For instance, LUKE required 30 days of training on 16 Nvidia V100's \cite{yamada2020luke}, and that model already took advantage of RoBERTa, which was even more demanding to train \cite{liu2019roberta}.
For this reason, the training of a language model is typically divided into two separate tasks:
The pretraining, in which a model is trained that has little practical purpose but has general language understanding and produce \emph{contextualized word representations} (CWR's) that will be introduced in section \ref{sec:transformers}, and a downstream training in which the pretrained model is fine-tuned on a much smaller dataset for some specific language task, e.g. named entity recognition.

Such pretrained models, producing CWR's, are called \emph{general-purpose language models} \cite[Ch. 2]{birk2020knowledge} and will in this project be referred to simply as \emph{language models}.

\section{Named Entity Recognition}
\begin{enumerate}
    \item Lidt flere eksempler på anvendelser? marrero og særligt danlp har begge en ret udtømmende liste af NER-ting
\end{enumerate}
Natural language processing has the broad goal of algorithmically understanding natural languages such as English and Danish.
To quantify this abstract ideal, understanding is often measured in performance on a set of well-defined language challenges that are expected to require some general understanding.
One such challenge is the task of recognizing \emph{named entities}, rigidly defined real-world objects such as specific persons, locations, organizations, events, products, etc.
The problem of named entity recognition (NER) is to produce an algorithm that can, given a string of characters designating a document of natural language, return an \emph{annotation} which ascribes, to each word in the document, both whether the word is a part of a named entity (NE) and what category of NE the word corresponds to \cite{wiki2021ner}.
For the string
\[
    \text{
        ''Caesar marched on Rome, defying the Senate of the Roman Republic.'',
    }
\]
a correct result could be
\begin{itemize}
    \item \emph{Caesar} is a person 
    \item \emph{Rome} is a location,
    \item \emph{the Senate of the Roman Republic} is an organization,
    \item and all other words are not part of named entities.
\end{itemize}
There are multiple ambiguities in this formulation of the problem but this is a common issue for the problem, as the set of NE types varies between benchmarks and as the definition of a NE is unsettled with possible answers including proper nouns, rigid designators and unique identifiers \cite[Sec. 4]{marrero2013ner}.
For the benchmarks used in this project, the NE categories and annotation formats are introduced in Section~\ref{sec:nerdata} while the evaluation of the performance of NER algorithms is discussed in Section~\ref{subsec:nereval}.

NER is not just used as a benchmark for language understanding:
In the applied field of information retrieval, semantic annotation and question answering systems rely on NER to control the information focus \cite[Sec. 2]{marrero2013ner}.
This task of both research and practical interest was defined in the 90's and has been active and competitive, especially in English where state of the art methods achieve close to human performance on some classic benchmarks \cite{wiki2021ner, marrero2013ner}.
Researchers argue, however, that the task is not yet solved as the generalization of the statistical algorithms fitted to one NER data set to new examples is poor \cite[Sec. 7.2]{marrero2013ner}.

The state of the art on English NER benchmarks moves quickly, but almost all algorithms currently (June 2021) achieving high scores in common benchmarks are deep neural networks and of the transformer architecture in particular \cite{ruder21ner, pwc21ner}.

\section{Embeddings, Tokenization and The Transformer}
\label{sec:transformers}
Danish is, compared to English, a low resource language:
The world Danish speaking population is smaller than the English speaking population of the largest cities, naturally resulting in significantly fewer and smaller available text datasets, and in less attention from both researchers and practitioners.
While pushing the performance of current NLP algorithms to their limits is much more difficult in low resource languages, the field of applied NLP in Danish has adopted many of the deep NLP developments of recent years.

Especially models based on the transformer neural network architecture \cite{vaswani2017att} currently dominate the Danish NER scene \cite{danlp2021}.
To understand the transformer, a closer look is first taken at traditional word embeddings and their shortcomings.

\paragraph{Word embeddings}
Word embeddings are one of the most ubiqitous tools in the NLP toolbox and come in many variants, all sharing the general idea of mapping a word token to a real-valued vector representation corresponding to a semantic condensation of the word.
When using the term \emph{embeddings}, we refer to the static, uncontextualized representations such as the \emph{fasttext}-based, 300-dimensional, Danish embeddings released by Edouard Grave et al. \cite{grave2018learning}.

One of the challenges in language modelling is that many words change their meaning completely depending on the context in which they appear.
Consider for instance, the sentences "Musen jagtede elefanten" and "Musen blev brugt til at styre computeren".
"Musen" ("the mouse" in English) appears identically in both sentences, but with widely different meanings, which embeddings have a hard time modelling.
In the latent space of the embeddings, one would expect "musen" to be close to other species from the animal kingdom, but also close to computer parts.
This would in turn cause computer parts to be placed close to animals, which is not ideal.
This issue with embeddings is where the transformer comes in.

%TODO Eksempler på embeddings og CWRs af "musen"

\paragraph{The Transformer}
Introduced by Ashish Vaswani et al. in 2017 \cite{vaswani2017att}, the goal of the transformer is to produce \emph{contextualized word representations} (CWR) given static, but learned embeddings.
CWR's are similar to embeddings in that they are real-valued vector representations of words that take into account the context.

Transformers are not the first attempt to solve this problem of producing effective CWR's.
Older architectures such as recurrent neural networks \cite[Ch. 10]{Goodfellow-et-al-2016} and long short-term memory networks \cite{hochreiter1997lstm} have yielded good results, but they have three weaknesses: First of, they only include context up to the given word, and secondly, they struggle with long-term dependencies due to their sequential nature resulting in poor information preservation \cite{Goodfellow-et-al-2016}.
Finally, the sequentiality of their calculations also limits how parallelized training and inference can be.
The first issue has been solved by the introduction of bi-directional versions of these network types \cite{schuster1997birnn}.
The second issue of long-term dependencies and the third of parallelizability is what the transformer seeks to tackle.

Both of these issues are solved by the attention mechanism.
Instead of sequentially letting each token update the state as in traditional language models, the relevance of all tokens to all other tokens are calculated simultaneously, which can be done in parallel while not loosing any information to an iterative state update.

The transformer uses an encoder-decoder structure with the encoder producing the CWR's.
The encoder is made of several attention blocks feeding into each other with feed forward networks and layer normalizations in between.
The attention blocks are each made up of three learnable matrices: the query, key, and value matrices.
The word embeddings are structured in a single matrix with token embeddings in the rows and multiplied upon the three matrices before the attention scores calculated.
This allows every token to be considered simultaneously, independently, and equally, solving both the described issues.
\cite{vaswani2017att}

Models based directly on the transformer have been trained on very large corpora to produce English language models such as BERT \cite{devlin2019bert}, RoBERTa \cite{liu2019roberta} and GPT-3 \cite{brown2020language} that have achieved state of the art in several downstream language tasks.
%TODO Skal der gives mere plads til at forklare BERT?
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{imgs/transformer}
    \caption{The transformer model as introduced by Vaswani et al. \cite{vaswani2017att}}
    \label{fig:transformer}
\end{figure}\noindent

\paragraph{Tokenization}
So far, all discussion regarding tokenization has simply regarded all unique words in the corpus as tokens.
However, this is not an optimal way to tokenize text due to how many forms words can exist in.
Just consider, for instance, how many ways a verb can be conjugated.
It still carries much of the same meaning, but an embedding will have to be learned for every single conjugation.
One way to handle this is through lemmatization where words are reduced to their stems.
However, this method is prone to information loss.
A more efficient way of tokenizing has been presented by Sennrich et al. \cite{sennrich-etal-2016-neural} that uses the byte-pair encoding (BPE) compression algorithm.
This method produces a token vocabulary of a given size from the corpus where each token is either a full word or a sub-word.
BERT uses the WordPiece BPE token vocabulary which contains 30,000 tokens that have been learned with heuristic tokenization rules \cite{wu2016tokenize} \cite{devlin2019bert}.
Radford et al. \cite{Radford2019gpt2} modify BPE tokenization for the GPT-2 transformer such that splits are made on the byte level rather than on the character level, which works better on corpora containing many non-ASCII characters.
This method is used by RoBERTa to produce a token vocabulary of size 50,000 \cite{liu2019roberta}.
LUKE directly uses RoBERTa's tokenizer \cite{yamada2020luke}.

\section{Deep Natural Language Processing in Danish}
\label{sec:nlpda}
As mentioned earlier, static embeddings are a core part of many NLP pipelines and have been for a long time.
In the large, open source NLP frameworks \emph{SpaCy} \cite{honnibal2020spacy} and \emph{Flair} \cite{akbik2019flair}, which also release pretrained Danish models, the embeddings interact with different, ever-improving language models.
This has successfully resulted in state of the art on multiple Danish NLP tasks.
The Alexandra Institute, a Denmark based AI research and development organization, release versions of both Flair and SpaCy finetuned on a number of benchmarks, including NER, and acquire close to state of the art performance using these.
The benchmarks, results and finetuned, Danish models are released as a part of their open source DaNLP project gathering resources for Danish NEk\cite{danlp2021}.
Another application of exsting NLP frameworks was performed by Kenneth Enevoldsen, Center of Humanities Computing, Aarhus University in releasing the model \emph{DaCy}, based on version 3 of SpaCy \cite{enevoldsen2020dacy}.

In Danish, the most significant, pretrained language model is in our estimation a Danish version of BERT, released by the company BotXO in 2019 \cite{botxo2019dabert} .
This model, in the following called \emph{da-BERT}, is produced by pretraining the base version of the very influential architecture \emph{Bidirectional Encoder Representations from Transformers} (BERT) \cite{devlin2019bert} on 10 GB of Danish text predominantly acquired from web scrapes.
The da-BERT model has been finetuned on most Danish benchmarks in the DaNLP project and consistently yield top results \cite{danlp2021, hvingelby2020dane}.

Another, more recent, Danish transformer is the adaption of the more resource efficient language model Electra to Danish under the name Ælæctra by Malte Hømark-Bertelsen, KMD and Aarhus University \cite{bertelsen2020lctra}.
NER finetuned versions of Ælæctra and some multilingual transformers have been released by Ekstra Bladet Analyse under the project \emph{NER for Danish} (NERDA) \cite{kjeldgaard2020nerda}.

Of this plethora of Danish language models and NLP frameworks, the current (June 2021) state of the art (SOTA) on central NER benchmarks is, according to DaNLP, the 2021 newcomer DaCy which surpassed DaNLP's fine-tuned da-BERT model which was SOTA at the beginning of this project \footnotemark.
\footnotetext{
    A table of the results can be seen at the DaNLP repository at \url{https://github.com/alexandrainst/danlp} under \code{docs/docs/tasks/ner.md}.
}
Reproduction of these results and comparisons with DaLUKE results is one of the goals of this project and the NER benchmark will be presented in Section \ref{subsec:daNERdata}, the reproduction method in Section \ref{sec:exidan}, and the results in Section \ref{sec:nerres}.

Finally, the NLP scene of a language is much more than the public, pretrained models:
The available rule-based tools, corpora and other language resources are instrumental in developing practical NLP pipelines.
One pipeline supplying multiple of these resources is the IT University of Copenhagen project DKIE, also including a NER model \cite{derc2014dkie}.
A 2019 paper by NLP researchers at the same university surveyed the availability of such tools and highlight a lack of practical resources \cite{kirkedal2019lacunae}.
Other overviews of Danish NLP resources include those published by the Alexandra Institute at DaNLP \cite{danlp2021} and the comprehensive list maintained by Finn Årup Nielsen, DTU Compute \cite{arup21awesome}.

\section{Deep, Knowledge-enhanced NLP}
The combination of statistical methods and explicit human crafted domain knowledge was the state of the art in many natural language processing (NLP) tasks in the 1990's and up into the 2000's \cite[Sec. 22.5]{russell2016ai}.
As with most other fields of AI, the emergence of performant deep learning methods disrupted NLP in the 2010's, popularizing the methodology of using deep architectures, generally reccurent neural networks or transformers, as general purpose language models that can be fine tuned to a number of different tasks \cite{otter18dlnlp}.
This approach rarely includes any modelling of knowledge that is not induced implicitly by the contextual representations generated during pretraining on an unannotated text corpus.

The Defense Advanced Research Projects Agency (DARPA) identify the future wave of AI as one of \emph{contextual adaption}; a combination of deep learning for perception and latent representations and symbolic modelling methods \cite{darpa17ai}.
In NLP, knowledge-enhancing deep neural networks is not just an idea for the future:
Several approaches using explicit knowledge to improve pretrained contextual word representations (CRW's) have been presented in recent years, a subset of which will be summarized here.

\subsection{Static, separate knowledge graphs}
A direct way to use explicit knowledge is to maintain a separate representation of facts which can be incorporated into both pretraining and inference of deep language models.
This representation is often called a knowledge base (KB)  -- or knowledge graph (KG) if the relational facts or entities are modelled.

In 2019, \emph{Enhanced Language Representation with Informative Entities}, ERNIE, was introduced by a Beijing team \cite{zhang2019ernie} looking to ''enhance language representation  with  external  knowledge'' \cite[1]{zhang2019ernie} by recognizing mentions of named entities in given text and retrieving their positions in a separate knowledge graph.
The query from the knowledge graph is encoded into knowledge embeddings which are taken as input for a BERT-based dual transformer architecture.
The model was during training required to fill in randmoly masked named entities in given sequences using the KG.
ERNIE was not evaluated on named entity recognition in the original paper, but was evaluated on the \emph{General Language Understanding Evaluation} benchmark \cite{wang2018glue} where it did not outperform BERT.

Later in 2019, a similar idea was proposed in \emph{Knowledge Enhanced Contextual Word Representations} where the model \emph{KnowBert} \cite{peters2019knowbert} was shown to outperform ERNIE and BERT in knowledge related tasks, though GLUE or NER was not tested.
Here, the key addition was the Knowledge Attention and Recontextualization (KAR) component which allows information to retrieved from multiple KG's of different forms such as WordNET and Wikipedia and represented within the BERT encoder.

In KnowBert, though the KAR weights were trained, the KG was only used for encoding fixed facts for the model.
The \emph{Knowledge Graph Language Model} (KGLM) presented in \emph{Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling} \cite{logan2019barack} employs a more dynamic use of the knowledge graph.
The model uses the same approach to representing knowledge graph facts as ERNIE, but also builds a local graph on the sequence level which is grown by a generative model with each new token.
The paper presents improved fact completion compared to GPT-2 and further analysis has shown that the overall fact completion of KGLM is comparative to KnowBert, though they perform very differently from domain to domain, with KGLM being most dependant on the used knowledge graph \cite{birk2020knowledge}.

\subsection{Pretraining augmentations}
A strength of the statistical, deep approach that implicitly models knowledge is the potential for generalizable results:
You do not need a new domain specific KG for every application if enough general knowledge has been caught in your weights.
Motivated by the hope of this elusive generalizability, a number of methods have been proposed to enhance specifically the pretraining of language models to better have handle factual knowledge.
LUKE, the subject of this project, is one of these attempts.

An example of this weaker knowledge modelling is found in the 2019 model \emph{KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation} \cite{wang2021kepler} where downstream inference can be performed without any additional resources than the transformer structure itself.
The knowledge enhancement of this BERT-based language model is, however, in the pretraining where the model optimizes over a joint task of both performing masked language modelling (MLM) and a novel knowledge graph objective requiring the model to learn knowledge embeddings.
KEPLER does not improve GLUE performance from RoBERTa \cite{liu2019roberta} from which it was initialized, but beats both classical language models and knowledge enhanced ones such as ERNIE and KnowBERT on a number of knowledge related tasks.

This idea of pretraining a transformer both for MLM and for a new knowledge-guided task has been taken up multiple times, including in LUKE.
\emph{WKLM}, presented in 2019 in \emph{Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model} \cite{xiong2019wklm}, uses the English Wikipedia, taking hyperlinks to be entities, and trains an unaltered BERT architecture jointly on MLM and on a task involving identifying the randomly replaced entity in a sequence.
\emph{KALM}, published after LUKE in \emph{Knowledge-Aware Language Model Pretraining} \cite{corby2020kalm}, also keeps the core language model architecture, in this case GPT-2, unchanged, adding a separate entity tokenizer and entity embedding layer for an entity prediction pretraining task.

Though these models achieve good performance, with WKLM surpassing BERT and ERNIE on some question answering and entity related benchmarks, and KALM outperforming GPT-2 on some knowledge related zero-shot tasks, the 2020 article \emph{K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters} \cite{wang2020kadapter} raises a concern with the approach.
According to the authors, the idea of knowledge-augmenting the pretraining task and then updating the entire model limits the ability to inject versatile knowledge and sets up a heterogeneous learning problem resulting in entangled parameters that are hard to investigate.
The model \emph{K-Adapter} instead uses ''adapters'', add-ons for the transformer structure, which are pretrained for different knowledge tasks while the base RoBERTa parameters are not updated.
The resulting model outperforms BERT, WKLM, ERNIE, KEPLER and KnowBERT on central knowledge related tasks.

Another issue, raised more recently, with this jungle of knowledge enhanced models is that they do not unify generative and discriminative tasks, limiting their generalizability to fewer downstream tasks.
This is voiced in \emph{KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning} \cite{he2020kgplm} where the model KgPLM, pretrained by both generating masked entities and discriminating replaced ones, is proposed as a solution.

Which of the knowledge-enhanced models result in the best general performance for practitioners is beyond the scope of this project, as the continuous improvement of many of the models and large amount of different benchmarks used in the articles makes direct comparisons of general language understanding ability challenging.
On all benchmarks tested by the LUKE team \cite{yamada2020luke}, none of the above models have presented higher scores than LUKE \cite[Sec. 4]{yamada2020luke}.


\section{LUKE}
\label{sec:LUKE}
\begin{enumerate}
    \item VIGTIG: Gør dette mindre inforstået og introducér flere af de nye begreber
    \item LUKE's specielle egenskaber nedtones lidt for meget i slutningen af dette for-afsnit og ved at sige "only architectural difference"
    \item Gennemgå afsnit og sørg for, at det passer i sin nye kontekst
    \item Vis et eksempel nede i prætræningsafsnittet af, hvordan prætræningsdata ser ud.
    \item Kryptisk vægtinitialisering
\end{enumerate}
LUKE (Language Understanding with Knowledge-based Embeddings) is a language model introduced by Yamada et al. in November of 2020 \cite{yamada2020luke}.
As the model is tested and extended in this project, key properties of the architecture and pretraining methodology are presented in this section.

As other new models discussed previously, LUKE builds on the BERT transformer architecture \cite{devlin2019bert}, adding a knowledge-enhancing pretraining task.
But the LUKE encoder does not only produces contextualized word representations (CWR); 
\emph{entities} are also represented by the encoder by mapping each entity to the same latent space as the words.

Yamada et al. show that this approach leads to state of the art results on a number of entity-related tasks, such as NER, and argue that the LUKE approach mitigates the following issues with existing methods:
\begin{itemize}
    \item Existing CWR's cannot represent spans of multiple words requiring more downstream learning for language tasks in which word spans are important.
    \item While transformers are good at capturing relationships between single words, they have difficulty modelling such relationships between sequences of words, of which entities often consist.
    \item Finally, the word-based pretraining task is not suited for producing entity-level representations, as the masked language model (MLM) mostly learns to predict single words rather than entire entity spans.
        An example is predicting ''Rings'' in ''We watched Peter Jackson's The Lord of the [MASK]'', where predicting the single word is clearly easier than predicting the entire entity.
\end{itemize}
The LUKE idea is to consider entities as first-class citizens in language modelling along with words, maintaining vocabularies of both entity and word tokens.

\subsection{Architecture}
LUKE is a transformer that operates on input sequences using trainable word and entity embeddings which are encoded by 24 self-attention transformer layers.
% \begin{figure}[H]
%     \centering
%         \includegraphics[width=\linewidth]{}
%     \caption{}
%     \label{fig:}
% \end{figure}\noindent


After these, the model can be extended with a decoder consisting of bi-directional classifications heads for pretraining, or it can be extended with linear layers for downstream tasks.

\subsubsection{BERT Architecture}
The word/entity duality of LUKE means that a large part of the model performs the same task as the conventional word-based language models.
For this reason, the word embeddings in LUKE follow the BERT architecture and Yamada et al. initialize these embeddings to those found in RoBERTa.
In the pre-training of Yamada et al., the encoder of LUKE is also equivalent with that of BERT and is initialized the weights of RoBERTa.

\subsubsection{Entity Embeddings}
The only architectural difference between LUKE and BERT in the pretraining of Yamada et al. (apart from pretraining specific heads) is the addition of a map from entity id's and entity position id's to entity embeddings.
This part of the model consists of lookup tables that store values for each possible id.
For the entity token id embedding, the size of the lookup table corresponds to the size of the entity vocabulary.

The entity embeddings are concatenated to the word embeddings and are passed through the encoder such that the two token domains exist in the same latent space.
After the encoding, the output can be split into representations for the words and entities in the sequence.

\subsubsection{Entity-aware Self-attention}
\label{subsubsec:entityaware}
Yamada et al. present a entity-related change to the BERT encoder architecture in the query metchanism of the attention scorer \cite[Sec. 3.2]{yamada2020luke}.

For the attention between token $i$ and token $j$ with the hidden states $\mathbf x_i, \mathbf x_j$, a core part of the normal transformer attention mechanism is to compute the following scalar:
\begin{equation}
    q_{ij} = \mathbf x_j^\top \mathbf Q \mathbf x_i,
\end{equation}
where $\mathbf Q\in \RR^{H\low{in}\times H\low{out}}$ is called the query matrix for a layer with input hidden size $H\low{in}$ and output hidden size $H\low{out}$ \cite[Sec. 3.2.1]{vaswani2017att}.
% FIXME: H_in og H_out må da være det samme?

In LUKE, the tokens $\mathbf x_i$ might either be words or entities.
To handle this explicitly, entity-aware self-attention changes the computation of the scalar $q_{ij}$ to
\begin{equation}
    q_{ij} = 
    \begin{cases}
    \mathbf x_j^\top \mathbf Q_{w2w} \mathbf x_i  & \text{if both $\mathbf x_i$ and $\mathbf x_j$ are word tokens}\\
    \mathbf x_j^\top \mathbf Q_{w2e} \mathbf x_i & \text{if $\mathbf x_i$ is word and $\mathbf x_j$ is entity}\\
    \mathbf x_j^\top \mathbf Q_{e2w} \mathbf x_i & \text{if $\mathbf x_i$ is entity and $\mathbf x_j$ is word}\\
    \mathbf x_j^\top \mathbf Q_{e2e} \mathbf x_i & \text{if both $\mathbf x_i$ and $\mathbf x_j$ are entity tokens}
    \end{cases}
\end{equation}
LUKE, however, is not pretrained using this mechanism, but for the fine-tuning tasks, Yamada et al. show in ablation experiments that this addition consistently yields better performance \cite[Sec. 5.2]{yamada2020luke}.

\subsection{Pretraining}
Yamada et. al perform the pretraining of LUKE by extending the MLM task of BERT \cite{devlin2019bert}.
Where BERT is trained to predict randomly masked words, LUKE is trained to predict both randomly masked words and entities.

The pretraining task thus gives three sequences as input:
Sub-word token id's, entity token id's, and positions of the entities.
Following RoBERTa \cite{liu2019roberta}, sub-word tokens making up 15\pro\ of full words in each sequence are randomly replaced with the [MASK] token.
Of these, 10\pro\ are unmasked again, and another 10\pro\ are replaced by a random token from the vocabulary instead of [MASK].
Similarly, 15\pro\ of entity tokens are replaced with [MASK], but none of these are unmasked or replaced with random entities.
The model is then trained to predict all sub-word tokens 
From these, the model is to classify the tokens at the mask positions in both sequences.
The model parameters are then updated by the stochastic gradient optimization algorithm known as \emph{Adam with weight decay fix} (AdamW) \ref{loshchilov2019decoupled}.

For a batch of size $ N $ and $ C $ classes, the model produces a matrix of size $ N\times C $ where $ X_{i, j} $ is the class score for class $ j $ for the $ i $'th example in the batch.
For each of the two classification tasks, the cross entropy loss $ l $ is then calculated as
\begin{equation}\label{eq:crossentropyloss}
    l = \frac{1}{N} \sum_{i=1}^N \left(
        -X_{i, c_i} + \log \sum_{j=1}^C \exp X_{i, j}
    \right)
\end{equation}
where $ c_i $ is the true class of the $ i $'th example \cite{pytorchcel}.
The total loss is then calculated as the sum of individual losses for both tasks.

Yamada et al. \cite{yamada2020luke} use weight decay for the entire model barring bias and layer normalizations.
According to Krogh and Hertz \cite{krogh1991weight}, weight decay improves generalizability in feed-forward networks.
This is due to the suppression of irrelevant parameters, letting the smallest possible parameters solve the problems.
Larger parameters naturally lead to larger sensitivity to noise and thus poorer generalization.

\subsubsection{Entity Mask Prediction Head}
For the entity pretraining task, LUKE is equipped with another classifier structure in addition to the masked language model prediction head inherited from BERT.
This new prediction head follows the architecture of the masked word scorer just operating on the entity representations.
The masked entity tokens are thus scored as corresponding to one of the entities in the entity vocabulary by two linear layers between which an activation function and layer normalization are placed.

\subsection{Finetuning for Named Entity Recognition}
\label{subsec:finetuning-theory}
Yamada et al. base their NER fine-tuning on Sohrab and Miwa \cite{sohrab2018nestedner}.
All possible entity spans (or $n$-grams) up to length 16 for computational efficiency over a tokenized text piece are calculated.
Entity spans that cross sentence boundaries are exluded.
These are considered the named entity candidates and are given to the pretrained transformer as entity inputs along with the subword tokens from the text piece.
Named entity candidates are given as [MASK] tokens in direct continuation of the pretraining, where the model learned to predict those tokens.
As such, there are only two entity input ID's in the fine-tuning: [PAD] and [MASK].
Embeddings for all other entities are discarded.
For each named entity candidate, the computed CER and the CWR's of the first and last tokens in the corresponding span are concatenated.
This combined representation is then given to a single linear layer that learns to classify the entity as either a non-entity or one of the classes in the dataset.
For a dataset with $ n $ classes, the learning problem is then an $ n+1 $ class classification task.
The loss is calculated as the cross entropy loss between the predicted class scores and the true class scores using equation \eqref{eq:crossentropyloss}.
Both the full pretrained model and the linear classifier are fitted.

An issue with the $ n $-grams is the large amount of overlaps between spans.
This is solved by greedily selecting spans based on the class scores from the classifier.
Thus, if two (partially) overlapping spans both are predicted to cover an entity, only the one with the highest score is kept, while the other is predicted to be a non-entity.

Following BERT \cite{devlin2019bert}, the maximal document context is included in the target document, allowing each entity prediction to use as much context as the data-set provides, limited only by the maximum of 512 sub-word tokens given to LUKE.
This plays to the strengths of the transformer, which is good at retaining attention over long sequences of text \cite{vaswani2017att}.
\cite{yamada2020luke}

\end{document}
