\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Methods}
\lhead{Methods}

\section{LUKE}
LUKE (Language Understanding with Knowledge-based Embeddings) is a language model introduced by Yamada et al. in November of 2020. \cite{yamada2020luke}.
While conteptually similar to models such as BERT \cite{devlin2019bert} and RoBERTa \cite{liu2019roberta}, LUKE not only produces contextualized word representations (CWR), but also contextualized entity representations (CER), which, as Yamada et al. show, leads to state-of-the-art results on a number of entity-related tasks.

BERT \cite{devlin2019bert} and RoBERTa \cite{liu2019roberta} yield strong, general-purpose CWR's that are effective for many different NLP tasks but are often suboptimal for entity-related tasks.
Yamada et al. suggest three reasons for this being the case.
\begin{itemize}
    \item Existing CWR's include no span-level representation of entities, so these will have to be learned from a often small downstream dataset.
    \item While transformers are good at capturing relationships between single words, they have difficulty modelling such relationsips between sequences of words, of which entities typically consist.
    \item Finally, the word-based pretraining task is not suited for producing entity-level representations, as the masked language model (MLM) usually only has to predict a single word in an entity given the rest of the entity.
    An example is predicting "Value" in "Deep Fucking [MASK]", where predicting the single word is clearly easier than predicting the entire entity.
\end{itemize}
%TODO: Phrase MLM better
LUKE, in contrast to BERT and RoBERTa, also considers entities as tokens, and it takes and inputs both word and entity tokens and produces both CWR's and CER's, respectively.
This allows LUKE to deal with the listed problems with word-based models.

Architecturally, LUKE has a single transformer on top of word- and entity embeddings that maps both types of tokens into the same latent space.

\subsubsection{Pretraining}
LUKE's pretraining is a direct extension of BERT's MLM. \cite{devlin2019bert}
Where BERT is trained to predict randomly masked words, LUKE is trained to predict both randomly masked words and entities.
An architectural overview of LUKE including the pretraining task is shown on Figure REF.

The model is trained over 200K steps.
Wikipedia pages are iterated over in random order.

The word-entity duality of LUKE means that a large subset of the model is similar to conventional word-based models.
For this reasons, all parameters in the transformer as well as word word embeddings are initialized to those found in RoBERTa.

The authors also introduce a new entity-aware self-attention mechanism -- a version of the original self-attention mechanism in transformers that takes entities into account.
This is not used for pretraining, however, and only for fine-tuning.

%%GAMMELT Ã†VL
% The model produces contextualized representations of both words and entities which can then be used for fine-tuning it for specific tasks.
% This is unlike other models such as BERT \cite{devlin2019bert} and RoBERTa \cite{liu2019roberta} which produce only contextualized word representations (CWR).
% This allows fine-tuned tasks using LUKE to use the knowledge gained from entities, ideally improving results.

% \subsection{Pretraining}
% BERT, RoBERTa, and similar models use only the token embeddings inferred directly from the given sequences when pretraining.
% LUKE, which uses RoBERTa's tokenizer, also receives this.
% However, the entities that Wikipedia provides as hyperlinks, are also embedded and fed to the model.

%Discuss: Here as motivation or later? https://towardsdatascience.com/gpt-3-has-no-idea-what-it-is-saying-95d4c1bad4a8
%TODO: Output representations
%TODO: Combination of LM and entity embeddings
%TODO: Pretraining, new pretraining task
%TODO: Posttraining
%TODO: Results here?
%TODO: Entity-aware self-attention
%TODO: Entity-related tasks
%TODO: Same latent space for words and entities
%TODO: Formality, normality ðŸŽµðŸŽµðŸŽµ

\section{Benchmarking Named Entity Recognition}
\subsection{Off-the-shelf, Danish models}
\label{sec:exidan}
A number of Danish NER models that are publily available and usable by NLP practitioners are collected and evaluated on the testing datasets of three Danish NER annotations considered in \ref{subsec:daNERdata}: DaNE, Plank and Wiki-ANN.

Most of the models are found through DaNLP\footnotemark, an open-source collection of Danish NLP software and references released by the Alexandra Institute which also released the DaNE dataset.
\footnotetext{
    The repository is at \url{https://github.com/alexandrainst/danlp} from which a collection of NER models can be found under \code{docs/docs/tasks/ner.md}.
}
\paragraph{DaNLP da-BERT}
is an NER model produced by DaNLP by using the pre-trained, Danish BERT, \emph{Bidirectional Encoder Representations from Transformers}, \cite{devlin2019bert} released by the company BotXO \cite{botxo2019dabert}.
The model is fine-tuned for NER on the DaNE test set \cite{hvingelby2020dane}.
\paragraph{NERDA m-BERT}
is a NER model fine-tuned by Ekstra Bladet Analyse in their repository NERDA\footnotemark, \emph{NER for Danish} using the pre-trained Multilingual BERT released by the original Google Research BERT team\cite{devlin2019bert}.
The training data was also DaNE.
\footnotetext{
    The repository is at \url{https://github.com/ebanalyse/NERDA}.
}
\paragraph{NERDA Ã†lÃ¦ctra}
is also released in the NERDA repository and uses the Danish Transformer Ã†lÃ¦ctra which has been pre-trained on the Danish Gigaword Corpus by Malte HÃ¸jmark-Bertelsen at KMD \cite{bertelsen2020lctra}.
The model is an adaption of the efficient BERT alternative Electra \cite{clark2020electra} and has been fine-tuned by NERDA on DaNE.

\paragraph{DaNLP Flair}
is a NER model based on the open-source contextual word embedding framework Flair \cite{akbik2019flair}, fine-tuned on DaNE by DaNLP.

\paragraph{DaNLP spaCy}
is a model released in DaNLP which is obtained by training the general, open-source NLP framework spaCy \cite{honnibal2020spacy} on the Universal Dependencies Danish Dependency Treebank (UD-DDT) Corpus\cite{johann2015udddt}.
This Danish spaCy model was then fine-tuned for NER on DaNE.

\paragraph{Polyglot}
is a NLP framework supporting a wide range of tasks in many languages including NER in 40 different languages.
The NER model is produced using automatic, language-agnostic annotations generated from Wikipedia and Freebase link structures \cite{rfou2015polyglot}.

\paragraph{DKIE Stanford CRF (daner)}
is an application of the Stanford CoreNLP Conditional Random Field (CRF) NER classifier \cite{manning2014corenlp} released by IT University of Copenhagen (ITU).
The model was trained on NER annotations produced at ITU on the Danish Dependency Treebank (DDT) corpus \cite{kromann2003ddt} as a part of the DKIE project \cite{derc2014dkie}.
The released Java-based NER tool is called \code{daner}\footnotemark.
\footnotetext{
    The repository is at \url{https://github.com/ITUnlp/daner}
}


\subsection{Fine-tuning English LUKE}
Yamada et al. benchmark \emph{LUKE}, their new transformer-based model for contextualized entity representations on a number of Natural Language Processing tasks, including Named Entity Recognition on the CoNLL-2003 dataset \cite{yamada2020luke}.

Reproduction of these results were performed by obtaining the pretrained LUKE models from Yamada et al.'s software repository.

The model was then fine-tuned for NER by training a linear classifier over all $n$-grams in each sentence with the named entities as targets for cross-entropy loss, as described in \cite[Sec. 4.3]{yamada2020luke}.
The same hyper-parameters as Yamada et al. were used and are shown along with and technical details in Table~\ref{tab:params}.

This procedure was repeated five times for each of the two released LUKE models, called \emph{large} and \emph{base}, to examine variability in the downstream training.

%%% Horrible footnote manipulation - it works, dont touch it! %%%
\addtocounter{footnote}{1}
\footnotetext{\label{foot:fnt}
    The pre-trained models were downloaded on 17/02-2021 from the LUKE software repository:
    \url{https://github.com/studio-ousia/luke/tree/6feefe657d97d2f847ace87f61f23b705f75d2aa\#released-models} 
} 
\begin{table}[H]
    \begin{tabular}{l|cc}
                                    & LUKE large & LUKE base\\\hline
        Pre-trained model
                                    & LUKE RoBERTa large \textsuperscript{\ref{foot:fnt}}\
                                                & LUKE RoBERTa base\textsuperscript{\ref{foot:fnt}}\\
        Pre-trained model parameters & $253\ctp 6$ & $483\ctp 6$\\
        Pre-trained model entity vocabulary & \multicolumn{2}{c}{$500\ctp3$}\\
        Learning rate               & $10^{-5}$ & $5\ctp{-5}$\\
        Batch size (effective)      & \multicolumn{2}{c}{8 (16)}\\
        Gradient accumulation steps & \multicolumn{2}{c}{2}\\
        Numeric precision           & \multicolumn{2}{c}{Mixed FP16/FP32 (Nvidia APEX)}\\
        Training code               & \multicolumn{2}{c}{PyTorch-based \code{luke}-repository \protect\footnotemark}\\
        Software version            & \multicolumn{2}{c}{Python 3.6, PyTorch 1.2}
    \end{tabular}
    \label{tab:params}
\end{table}

\footnotetext{
    The repository \url{github.com/studio-ousia/luke} was cloned at commit-SHA \code{6feefe6}, installed and used for the fine-tuning.
}


\section{daLUKE}



\end{document}
