\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Methods}
\lhead{Methods}

\section{Benchmarking Named Entity Recognition}
\subsection{Off-the-shelf, Danish models}
\label{sec:exidan}
A number of Danish NER models that are publily available and usable by NLP practitioners are collected and evaluated on the testing datasets of three Danish NER annotations considered in \ref{subsec:daNERdata}: DaNE, Plank and Wiki-ANN.

Most of the models are found through DaNLP\footnotemark, an open-source collection of Danish NLP software and references released by the Alexandra Institute which also released the DaNE dataset.
\footnotetext{
    The repository is at \url{https://github.com/alexandrainst/danlp} from which a collection of NER models can be found under \code{docs/docs/tasks/ner.md}.
}
\paragraph{DaNLP da-BERT}
is an NER model produced by DaNLP by using the pre-trained, Danish \emph{BERT}, Bidirectional Encoder Representations from Transformers, \cite{devlin2019bert} released by the company BotXO \cite{botxo2019dabert}.
The model is fine-tuned for NER on the DaNE test set \cite{hvingelby2020dane}.
\paragraph{NERDA m-BERT}
\paragraph{NERDA Ælæctra}
\paragraph{DaNLP Flair}
\paragraph{DaNLP spaCy}
\paragraph{Polyglot}
\paragraph{ITU Stanford CRF (daner)}

\subsection{Fine-tuning English LUKE}
Yamada et al. benchmark \emph{LUKE}, their new transformer-based model for contextualized entity representations on a number of Natural Language Processing tasks, including Named Entity Recognition on the CoNLL-2003 dataset \cite{yamada2020luke}.

Reproduction of these results were performed by obtaining the pretrained LUKE models from Yamada et al.'s software repository.

The model was then fine-tuned for NER by training a linear classifier over all $n$-grams in each sentence with the named entities as targets for cross-entropy loss, as described in \cite[Sec. 4.3]{yamada2020luke}.
The same hyper-parameters as Yamada et al. were used and are shown along with and technical details in Table~\ref{tab:params}.

This procedure was repeated five times for each of the two released LUKE models, called \emph{large} and \emph{base}, to examine variability in the downstream training.

%%% Horrible footnote manipulation - it works, dont touch it! %%%
\addtocounter{footnote}{1}
\footnotetext{\label{foot:fnt}
    The pre-trained models were downloaded on 17/02-2021 from the LUKE software repository:
    \url{https://github.com/studio-ousia/luke/tree/6feefe657d97d2f847ace87f61f23b705f75d2aa\#released-models} 
} 
\begin{table}[H]
    \begin{tabular}{l|cc}
                                    & LUKE large & LUKE base\\\hline
        Pre-trained model
                                    & LUKE RoBERTa large \textsuperscript{\ref{foot:fnt}}\
                                                & LUKE RoBERTa base\textsuperscript{\ref{foot:fnt}}\\
        Pre-trained model parameters & $253\ctp 6$ & $483\ctp 6$\\
        Pre-trained model entity vocabulary & \multicolumn{2}{c}{$500\ctp3$}\\
        Learning rate               & $10^{-5}$ & $5\ctp{-5}$\\
        Batch size (effective)      & \multicolumn{2}{c}{8 (16)}\\
        Gradient accumulation steps & \multicolumn{2}{c}{2}\\
        Numeric precision           & \multicolumn{2}{c}{Mixed FP16/FP32 (Nvidia APEX)}\\
        Training code               & \multicolumn{2}{c}{PyTorch-based \code{luke}-repository \protect\footnotemark}\\
        Software version            & \multicolumn{2}{c}{Python 3.6, PyTorch 1.2}
    \end{tabular}
    \label{tab:params}
\end{table}

\footnotetext{
    The repository \url{github.com/studio-ousia/luke} was cloned at commit-SHA \code{6feefe6}, installed and used for the fine-tuning.
}


\section{daLUKE}



\end{document}
