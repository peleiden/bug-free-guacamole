% !TeX spellcheck = en_GB
\documentclass[main.tex]{subfiles}

\begin{document}
\lhead{Methods}
\chapter{Results}
\lhead{Results}
\label{chap:results}

\section{English LUKE Reproduction}%
\label{sec:English LUKE Reproduction}
\begin{table}[H]
	\begin{center}
		\begin{tabular}{l l | c c c c c}
                    Model & Over 5 repetitions & Micro avg. & LOC & PER & ORG & MISC \\
			\hline
                    \multirow{2}{*}{LUKE large}& Mean F1 [\pro]& $94.0$ & $95.0$ & $97.2$ & $93.6$ & $85.2$ \\
                                               & Std. [\pro]& $0.2$  & $0.1$  & $0.1$ & $0.3$ & $1.0$ \\
                    \multirow{2}{*}{LUKE base} & Mean F1 [\pro]& $93.4$ & $94.6$ & $96.8$ & $92.4$ & $84.9$\\
                                               & Std. [\pro]& $0.2$ & $0.2$ & $0.2$ & $0.2$ & $0.7$
		\end{tabular}
	\end{center}
	\caption{
        Observed English LUKE results over five repetitions of fine-tuning and evaluating LUKE on CoNLL-2003 for each model size.
        }
	\label{tab:lukeF1s}
\end{table}
Yamada et al. report micro avg. F1 scores of 94.3\pro\ and 93.3\pro\ for LUKE large and base, respectively \cite{yamada2020luke}.
In both cases, the reported scores are within two standard deviations on observed F1 scores, which were estimated on training repetition.
This is expectable, and we conclude that the reproduction was successful.
However, the variability of fine-tuning is also highlighted here and will be discussed in Section~\ref{sec:fine-tuning-exp}.
% However, it also highlights the variability of the fine-tuning.
% With margins between top-performing models being as small as they are \cite{pwc21ner}, whoever can claim SOTA may well come down to who got the luckiest seeds.
% This could be mitigated by reporting the mean of several fine-tunings, but Yamada et al. at least do not mention having done multiple fine-tunings.

\section{Pretraining of DaLUKE}
\label{sec:Pretraining of DaLUKE}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{loss}
    \caption{
    Cross entropy loss throughout pretraining for both masking tasks as well as effective (weighted) loss, which is the average of the two.
    The loss on the entity masking (green) seems to drive close to all development in the average loss (blue) after the first few epochs.
    }
    \label{fig:loss}
\end{figure}\noindent

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{pretrain-acc}
    \caption{
    Masked words and masked entity accuracy throughout pretraining.
    The curves are smoothed with a rolling average.
    The frequency of exact correct word guesses quickly rests at 20-25\pro\ while the new task of entity masking starts from 0 and takes time to learn.
    }
    \label{fig:pretrain-acc}
\end{figure}\noindent
The loss shown on Figure \ref{fig:loss} mirrors the accuracy with masked word loss converging quickly, after which falling entity loss becomes the driving factor behind the falling total loss.
The masked words and masked entity accuracies are shown on Figure \ref{fig:pretrain-acc}.
The former converges quickly, achieving close to top accuracy in less than a tenth of the full training time, while the latter keeps improving throughout the training.
That the MLM performance from BERT is quickly maximized is, however, perhaps not surprising, as many of the relevant weights were initialized from an already trained model which has been pretrained on the same task.

\begin{table}[H]
    \centering
    \begin{tabular}{l|rrrrrr}
        Top $k$ accuracy [\pro] & $k=1$  & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$\\\hline
        Masked words            & 24.8       & 31.5      & 35.0      & 40.3       & 48.2       & 54.0      \\
        Masked entities         & 56.2       & 68.6      & 73.6      & 79.8       & 86.4       & 90.3
    \end{tabular}
    \caption{
        The main pretrained model performance in the last of 150 epochs.
        We note that the model has learned to guess masked entities consistently as the correct entity very often is one of the top 50 guesses.
    }
    \label{tab:mainpre}
\end{table}\noindent
\section{Danish Named Entity Recognition}%
\label{sec:nerres}

\subsection{Main Benchmark: Our Results and Reproduction}
              % precision    recall  f1-score   support

 % ┆   ┆   LOC     0.8365    0.9062    0.8700        96
 % ┆   ┆  MISC     0.7652    0.7273    0.7458       121
 % ┆   ┆   ORG     0.7956    0.6770    0.7315       161
 % ┆   ┆   PER     0.9441    0.9389    0.9415       180

 % ┆ micro avg     0.8467    0.8118    0.8289       558
 % ┆ macro avg     0.8354    0.8124    0.8222       558
% weighted avg     0.8440    0.8118    0.8262       558

              % precision    recall  f1-score   support

 % ┆   ┆   LOC     0.8365    0.9062    0.8700        96
 % ┆   ┆   ORG     0.7956    0.6770    0.7315       161
 % ┆   ┆   PER     0.9441    0.9389    0.9415       180

 % ┆ micro avg     0.8690    0.8352    0.8518       437
 % ┆ macro avg     0.8588    0.8407    0.8477       437
% weighted avg     0.8658    0.8352    0.8484       437
\begin{table}[H]
        \footnotesize
        \begin{center}
                \begin{tabular}{l l | c c c c | c c c c}
                    \multirow{2}{*}{Model name} & \multirow{2}{*}{Trained on} & \multicolumn{4}{c|}{Micro Avg. [\pro]} & \multicolumn{4}{c}{Class F1 [\pro]}\\
                    &         & F1                   & Prec.         & Rec.          & F1 {\tiny\textdiscount MISC} & LOC            & PER            & ORG            & MISC \\\hline
    DaLUKE        & DaNE      & 82.9 $\pm 3$         & 84.7          & 81.2          & 85.2         $\pm 3$         & \textbf{87.0} & \textbf{94.2} & 73.2          & 74.6 \\\hline
    DaNLP da-BERT & DaNE      & --                   & --            & --            & 84.0         $\pm 3$         & 83.9          & 92.8          & 73.0          & -- \\
    NERDA m-BERT  & DaNE      & 79.2 $\pm 3$         & 82.1          & 76.5          & 81.7         $\pm 4$         & 83.5          & 92.6          & 66.9          & 70.3 \\
    NERDA Ælæctra & DaNE      & 70.6 $\pm 4$         & 76.1          & 65.8          & 74.5         $\pm 4$         & 77.3          & 86.9          & 56.2          & 56.4 \\
    DaCy medium   & DaNE      & 78.3 $\pm 3$         & 78.3          & 78.3          & 80.5         $\pm 4$         & 84.0          & 90.4          & 66.2          & 70.1 \\
    DaCy large    & DaNE      & \textbf{84.9} $\pm 3$& \textbf{86.2} & \textbf{83.7} & \textbf{86.9}$\pm 3$         & 85.3          & \textbf{94.2} & \textbf{79.0} & \textbf{78.1} \\
    DaNLP spaCy   & DaNE      & 73.8 $\pm 4$         & 76.1          & 71.5          & 75.7         $\pm 4$         & 76.0          & 87.8          & 59.6          & 66.1 \\
    DaNLP Flair   & DaNE      & --                   & --            & --            & 81.8         $\pm 4$         & 84.8          & 93.2          & 63.0          & -- \\
    Polyglot      & Wikipedia & --                   & --            & --            & 64.2         $\pm 4$         & 65.0          & 78.7          & 39.3          & -- \\
    daner         & ITU DDT   & --                   & --            & --            & 56.5         $\pm 5$         & 59.4          & 70.4          & 28.3          & --
                \end{tabular}
        \end{center}
        \caption{
            Results of Danish NER models of the DaNE \cite{hvingelby2020dane} testing dataset consisting of 565 sentences.
            $\pm$ on the average F1's is notation for the approximate $95\pro$ confidence interval calculated using eq. \eqref{eq:ci}.
        Missing numbers are due to some models not being trained on the MISC annotations.
        We see the transformer-based models dominate the scores, spearheaded by the large implementation of DaCy.
        }
        \label{tab:DaNE}
\end{table}\noindent

\begin{figure}[H]
    \centering
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=\textwidth]{loss-fine}
    \end{minipage}\hfill
    \begin{minipage}{.49\textwidth}
        \includegraphics[width=\textwidth]{dev-dists}
    \end{minipage}
    \caption{
        To the left, the development of loss and accuracy on the DaNE training and dev. sets.
        Almost all learning happens in the first few epochs, hinting that the pretrained model already holds much of the language knowledge used for NER.
        To the right, the number of predictions of each class on the dev. set is shown along with the actual number of each class where MISC and ORG initially are under-predicted.
    }
    \label{fig:main-fine-tune}
\end{figure}\noindent
We conclude, when comparing with the June 2021 DaNLP NER table\footnotemark \cite{danlp2021}, that the results of all NER results previously reported on DaNE are successfully reproduced with most  scores corresponding exactly and a few varying within a percentage point.
\footnotetext{
    Table is available at \url{github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/ner.md} and was compared on 27/6, 2021 at commit SHA \code{584d5c7}.
}
Taking these results at face value, DaLUKE fails to achieve SOTA on DaNE, being beaten only by DaCy large but surpassing da-BERT, on which it is based.
However, when considering the approximate uncertainty estimates, the small test set makes these conclusions baseless, instead identifying a \emph{group} of transformer-based NER models leading the scoreboard.

Still, the considerable improvements in some categories when comparing to da-BERT is taken as an encouragement, suggesting that the extra pretraining and entity understanding indeed has added some knowledge to the model.
The improvements of DaLUKE compared to da-BERT are examined further in the following chapter.
Additionally, it is noted from considering the differences between algorithm performance across classes that future analysis of DaLUKE should examine how entity class performance is related to article bias in the Wikipedia pretraining dataset.

Ultimately, DaLUKE performs at a high level and shows promise for entity modeling.

\subsection{Additional Datasets}
              % precision    recall  f1-score   support

 % ┆   ┆   LOC     0.8298    0.8041    0.8168        97
 % ┆   ┆  MISC     0.0849    0.3000    0.1324        30
 % ┆   ┆   ORG     0.4710    0.6915    0.5603        94
 % ┆   ┆   PER     0.8994    0.9527    0.9253       169

 % ┆ micro avg     0.6054    0.8026    0.6902       390
 % ┆ macro avg     0.5713    0.6871    0.6087       390
% weighted avg     0.7162    0.8026    0.7493       390
              % precision    recall  f1-score   support

 % ┆   ┆   LOC     0.8298    0.8041    0.8168        97
 % ┆   ┆   ORG     0.4710    0.6915    0.5603        94
 % ┆   ┆   PER     0.8994    0.9527    0.9253       169

 % ┆ micro avg     0.7397    0.8444    0.7886       360
 % ┆ macro avg     0.7334    0.8161    0.7675       360
% weighted avg     0.7688    0.8444    0.8008       360
\begin{table}[H]
        \footnotesize
        \begin{center}
                \begin{tabular}{l l | c c c c | c c c c}
                    \multirow{2}{*}{Model name} & \multirow{2}{*}{Trained on} & \multicolumn{4}{c|}{Micro Avg. [\pro]} & \multicolumn{4}{c}{Class F1 [\pro]}\\
                                      &                & F1             & Prec.        & Rec.           & F1 {\tiny\textdiscount MISC} & LOC & PER & ORG & MISC \\\hline
                        DaLUKE        & DaNE           & \textbf{69.0} $\pm 5$& \textbf{60.5} & 80.3          & 78.9         $\pm 4$& \textbf{81.7} & \textbf{92.5}   & 56.0          & 13.2 \\\hline
                        DaNLP da-BERT & DaNE           & --                   & --            & --            & \textbf{79.2}$\pm 4$& 78.6          & 93.4            & \textbf{56.9} & -- \\
                        NERDA m-BERT  & DaNE           & 66.4          $\pm 5$& 58.1          & 77.4          & 76.6         $\pm 4$& 76.3          & 92.1            & 52.5          & 12.4 \\
                        NERDA Ælæctra & DaNE           & 66.3          $\pm 5$& 60.0          & 74.1          & 76.1         $\pm 4$& 74.9          & 90.3            & 53.0          & 13.2 \\
                        DaCy medium   & DaNE           & 65.6          $\pm 5$& 55.7          & 79.7          & 75.0         $\pm 4$& 76.1          & 92.1            & 48.7          & 12.6 \\
                        DaCy large    & DaNE           & 68.5          $\pm 5$& 58.9          & \textbf{81.8} & 79.0         $\pm 4$& 79.0          & \textbf{92.5}   & 58.0          & \textbf{15.5} \\
                        DaNLP spaCy   & DaNE           & 64.1          $\pm 5$& 55.9          & 75.1          & 72.7         $\pm 5$& 72.7          & 88.3            & 46.5          & 12.3 \\
                        DaNLP Flair   & DaNE           & --                   & --            & --            & 76.2         $\pm 4$& 80.2          & 94.4            & 36.1          & -- \\
                        Polyglot      & Wikipedia      & --                   & --            & --            & 64.1         $\pm 5$& 69.7          & 78.4            & 24.7          & -- \\
                        daner         & ITU DDT        & --                   & --            & --            & 59.8         $\pm 5$& 58.2          & 73.6            & 26.1          & --
                \end{tabular}
        \end{center}
        \caption{
            Danish NER algorithm performance on the Plank \cite{plank2019neural} test set consisting of 565 sentences.
            These models fine-tuned on DaNE perform very poorly on the Plank MISC entities in particular, highlighting NE annotater disagreement.
        }
        \label{tab:Plank}
\end{table}

              % precision    recall  f1-score   support

 % ┆   ┆   LOC     0.7626    0.7085    0.7346      5242
 % ┆   ┆   ORG     0.6497    0.3347    0.4418      4078
 % ┆   ┆   PER     0.7239    0.7757    0.7489      4378

 % ┆ micro avg     0.7267    0.6187    0.6684     13698
 % ┆ macro avg     0.7121    0.6063    0.6418     13698
% weighted avg     0.7166    0.6187    0.6520     13698
\begin{table}[H]
        \footnotesize
        \begin{center}
                \begin{tabular}{l l | c c c | c c c }
                    \multirow{2}{*}{Model name} & \multirow{2}{*}{Trained on} & \multicolumn{3}{c|}{Micro Avg. [\pro]} & \multicolumn{3}{c}{Class F1 [\pro]}\\
                        & & F1 & Prec. & Rec. & LOC & PER & ORG \\
                        \hline
                        DaLUKE          & DaNE      & \textbf{66.8}$\pm 0.1$ & \textbf{72.7} & 61.9          & 73.5          & 74.9          & 44.2 \\\hline
                        DaNLP da-BERT   & DaNE      & 65.7         $\pm 0.1$ & 68.6          & 63.2          & 72.1          & 74.5          & 40.1 \\
                        NERDA m-BERT    & DaNE      & 63.4         $\pm 0.1$ & 61.3          & 65.7          & 70.7          & 76.9          & 48.4 \\
                        NERDA Ælæctra   & DaNE      & 48.7         $\pm 0.1$ & 48.3          & 49.0          & 56.6          & 69.9          & 24.0 \\
                        DaCy medium     & DaNE      & 60.1         $\pm 0.1$ & 58.4          & 61.8          & 70.7          & 73.7          & 39.3 \\
                        DaCy large      & DaNE      & 64.6         $\pm 0.1$ & 62.2          & \textbf{67.1} & \textbf{74.1} & \textbf{77.2} & \textbf{49.8} \\
                        DaNLP spaCy     & DaNE      & 59.6         $\pm 0.1$ & 58.5          & 60.6          & 68.7          & 71.6          & 38.8 \\
                        DaNLP Flair     & DaNE      & 65.0         $\pm 0.1$ & 70.7          & 60.2          & 70.1          & 74.4          & 43.7 \\
                        Polyglot        & Wikipedia & 62.9         $\pm 0.1$ & 66.3          & 58.2          & 72.4          & 69.2          & 35.3 \\
                        daner           & ITU DDT   & 46.6         $\pm 0.1$ & 51.5          & 42.5          & 56.2          & 54.5          & 14.8
                \end{tabular}
        \end{center}
        \caption{
            Results of Danish NER models on the WikiANN \cite{pan2017wikiann, rahimi2019transfer} test set consisting of 10,000 sentences.
            This large dataset containing $\sim 14\ctp3$ entities results in small finite sample uncertainties on model performance making DaLUKE appear as a clear winner on this Wikipedia-based dataset.
        }
        \label{tab:WikiANN}
\end{table}
When fine-tuned on DaNE, DaLUKE achieves SOTA on WikiANN and slightly tops DaCy large on Plank.
This is unexpected given that DaLUKE was beaten on the DaNE test set.
It may indicate a better generalization ability, which, if true, is indeed a desirable property.
However, less exciting reasons for this behaviour may well be more likely.
With Plank, the difference is minuscule and clearly within random error.
The performance discrepancy on WikiANN appears surely significant, but may have an even less interesting reason:
DaLUKE is pretrained directly on similar, annotated Wikipedia data, compromising the principle that the training and test sets should be strictly disjoint.

\end{document}
