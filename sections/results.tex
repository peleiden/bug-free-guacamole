\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Results}
\lhead{Results}

\section{Reproductions}
\subsection{Danish NER benchmarks}
\begin{table}[H]
	\begin{center}
		\begin{tabular}{l l r r r r r r}
			Model & Train data & Micro avg. & w/o MISC & LOC & PER & ORG & MISC \\
			\hline
			BERT &  & 73.59 & 84.04 & 83.90 & 92.82 & 72.98 & - \\
			daner &  & 48.88 & 56.52 & 59.49 & 70.40 & 28.29 & - \\
			Flair &  & 71.12 & 81.78 & 84.82 & 93.15 & 62.95 & - \\
			mBERT &  & 79.22 & 81.71 & 83.50 & 92.61 & 66.90 & 70.34 \\
			Polyglot &  & 55.78 & 64.18 & 64.95 & 78.74 & 39.30 & - \\
			spaCy &  & 73.75 & 75.73 & 75.96 & 87.87 & 59.57 & 66.06 \\
			Ælæctra &  & 70.58 & 74.54 & 77.32 & 86.93 & 56.18 & 56.39 \\
			 &  &  &  &  &  &  &  \\
			Support &  & 558 & 437 & 96 & 180 & 161 & 121 \\
		\end{tabular}
	\end{center}
	\caption{F1\pro-scores of Danish NER models of the DaNE data-set consisting of 565 sentences.}
	\label{tab:DaNE}
\end{table}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{l l r r r r r r}
			Model & Train data & Micro avg. & w/o MISC & LOC & PER & ORG & MISC \\
			\hline
			BERT &  & 76.27 & 79.23 & 78.64 & 93.45 & 56.88 & - \\
			daner &  & 57.42 & 59.89 & 58.16 & 73.63 & 26.09 & - \\
			Flair &  & 73.16 & 76.16 & 80.21 & 94.35 & 36.96 & - \\
			mBERT &  & 66.37 & 76.60 & 76.33 & 92.08 & 52.53 & 12.41 \\
			Polyglot &  & 61.56 & 64.10 & 69.74 & 78.38 & 24.69 & - \\
			spaCy &  & 64.11 & 72.70 & 72.73 & 88.33 & 46.51 & 12.31 \\
			Ælæctra &  & 66.28 & 76.09 & 74.87 & 90.32 & 53.00 & 13.24 \\
			 &  &  &  &  &  &  &  \\
			Support &  & 390 & 360 & 97 & 169 & 94 & 30 \\
		\end{tabular}
	\end{center}
	\caption{F1\pro-scores of Danish NER models of the Plank data-set consisting of 565 sentences.}
	\label{tab:Plank}
\end{table}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{l l r r r r}
			Model & Train data & Micro avg. & LOC & PER & ORG \\
			\hline
			BERT &  & 65.66 & 72.08 & 74.49 & 40.05 \\
			daner &  & 46.59 & 56.15 & 54.51 & 14.76 \\
			Flair &  & 65.04 & 70.08 & 74.36 & 43.67 \\
			mBERT &  & 63.40 & 70.72 & 76.87 & 48.35 \\
			Polyglot &  & 61.99 & 72.45 & 69.15 & 35.33 \\
			spaCy &  & 59.55 & 68.68 & 71.63 & 38.76 \\
			Ælæctra &  & 48.65 & 56.63 & 69.89 & 24.02 \\
			 &  &  &  &  &  \\
			Support &  & 13698 & 5242 & 4378 & 4078 \\
		\end{tabular}
	\end{center}
	\caption{F1\pro-scores of Danish NER models of the WikiANN data-set consisting of 10000 sentences.}
	\label{tab:WikiANN}
\end{table}
\subsection{English LUKE NER performance}
\begin{table}[H]
	\begin{center}
		\begin{tabular}{l r r r r r}
			Model & Micro avg. & LOC & PER & ORG & MISC \\
			\hline
			LUKE large & $94.00 \pm  0.2$ & $94.99 \pm  0.09$ & $97.19 \pm  0.1$ & $93.63 \pm  0.3$ & $85.29 \pm  1$ \\
			LUKE base & $93.42 \pm  0.2$ & $94.67 \pm  0.2$ & $96.89 \pm  0.2$ & $92.41 \pm  0.2$ & $84.94 \pm  0.7$ \\
			 &  &  &  &  &  \\
			Support & 5616 & 1666 & 1602 & 1647 & 701 \\
		\end{tabular}
	\end{center}
	\caption{Mean F1\pro-scores and standard deviation over five repetitions of fine-tuning and evaluating LUKE on CoNLL2003 for each model size.}
	\label{tab:lukeF1s}
\end{table}

\end{document}
