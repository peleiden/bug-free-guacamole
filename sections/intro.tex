\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Introduction}
\lhead{Introduction}
% TODO: Gennemgå sætninger for, om der for meget friskhed i påstande?
% hvor meget skal der citeres?
The rapid development of artificial intelligence (AI) seen in the last decade is not just one of research;
the applied field sometimes called Data Science or AI Engineering has brought emerging intelligent technologies to industry, resulting in AI playing a significant role in society.  This data driven wave of AI has been led by the subfield that deals with statistical learning such as deep neural networks while symbolic methods using explicit knowledge have been less influential.
The successful societal impact of these statistical methods such as deep learning is, however, conditioned on the availability of big data and heavy computing resources resulting in high resource domains receiving most of the benefit of AI.

A clear example of this issue is the important field of natural language processing (NLP) that attacks the difficult task of understanding human language, one of the distinguishing aspects of our intelligence.
NLP based on statistical methods from AI has in recent years taken sizable strides towards this understanding with large-scale models such as GPT-2, GPT-3, BERT, RoBERTa, ELMo and T5 gaining public attention and seeing practical use.

The data hunger of these models results in natural languages with low amounts of available data, dubbed low resource languages, lacking severely behind in technical results and thus seeing less practical use of NLP.
To avoid these regions falling behind in the AI race, these languages, that include Danish, have seen a growing trend of attention from the literature resulting in a diverse set of methods attempting learning in the data sparse case \cite{hedderich2021survey}.

One such possible mitigation is to let the statistical approach be influenced by explicit modelling of knowledge.
In this project, language understanding in the low resource language Danish will be attempted using a new deep learning technique for NLP that succeeded with this modelling approach in English.
The model, \emph{Language Understanding Using Knowledge Embeddings} (LUKE), was released in October of 2020 and explicitly handles the difference between single words and named entities such as \emph{Lionel Messi}, \emph{The United States of America}, \emph{Science} \cite{yamada2020luke}.
The model had in total seen training from 160 GB of text\footnotemark and achieved state of the art results on a number of classic NLP tasks such as named entity recognition (NER).
\footnotetext{This includes all training that had an effect on the final model weights and thus also the data used to pretrain RoBERTa \cite[Sec. 3.2]{liu2019roberta}, the weights of which was used for initializing most of the LUKE weights. Counting only the data used for LUKE itself, the number is not directly reported but was found to be around 7 GB from our own experiments with the English LUKE data pipeline.}
For Danish, the amount of suitable data for this technique is about 40 times smaller \footnotemark.
\footnotetext{
    The pretraining method uses the special format of Wikipedia. The English Wikipedia dump used by LUKE contained 3.5 billion words \cite[App. A]{yamada2020luke} while the Danish Wikipedia consists of 81 million words.
}
It is the goal of this project to present the best possible open source Danish LUKE for use of Danish NLP practitioners while also taking this challenge as a case study of deep natural language processing for low resource languages.
% Nævn forskellen i compute?

\section{Introductory Theory and State of The Art}
\label{sota}

\subsection{Named Entity Recognition}
Natural language processing has the broad goal of algorithmically understanding natural languages such as English and Danish.
To quantify this abstract ideal, understanding is often measured in performance on a set of well-defined language challenges that are expected to require some general understanding.
One such challenge is the task of recognizing \emph{named entities}, rigidly defined real-world objects such as specific persons, locations, organizations, events, products, etc.
The problem of named entity recognition (NER) is to produce an algorithm that can, given a string of characters designating a document of natural language, return an \emph{annotation} which ascribes, to each word in the document, both whether the word is a part of a named entity (NE) and what category of NE the word corresponds to \cite{wiki2021ner}.
For the string
\[
    \text{
        ''Caesar marched on Rome, defying the Senate of the Roman Republic.'',
    }
\]
a correct result could be
\begin{align*}
    & \text{
        ''\emph{Caesar} is a person,
        \emph{marched} is not a NE,
        \emph{on} is not a NE,
        \emph{Rome} is a location,
    }\\
    &\text{
        \emph{defying} is not a NE,
        \emph{the Senate of the Roman Republic} is an organization''.
    }
\end{align*}
There are multiple ambiguities in this formulation of the problem but this is a common issue for the problem, as the set of NE types varies between benchmarks and as the definition of a NE is unsettled with possible answers including proper nouns, rigid designators and unique identifiers \cite[Sec. 4]{marrero2013ner}.
For the benchmarks used in this project, the NE categories and annotation formats are introduced in Section~\ref{sec:nerdata} while the evaluation of the performance of NER algorithms is discussed in Section~\ref{}.

NER is not just used as a benchmark for language understanding:
In the applied field of information retrieval, semantic annotation and question answering systems rely on NER to control the information focus \cite[Sec. 2]{marrero2013ner}.
This both of both research and practical interest was defined in the 90's and has been active and competitive, especially in English where state of the art methods achieve close to human performance on some classic benchmarks \cite{wiki2021ner, marrero2013ner}.
Researchers argue, however, that the task is not yet solved as the generalization of the statistical algorithms fitted to one NER data set to new examples is poor \cite[Sec. 7.2]{marrero2013ner}.

The state of the art on English NER benchmarks moves quickly, but almost all algorithms currently (June 2021) achieving high scores in common benchmarks are deep neural networks and of the transformer architecture in particular \cite{ruder21ner, pwc21ner}.

\subsection{Deep, Knowledge-enhanced NLP}
The combination of statistical methods and explicit human crafted domain knowledge was the state of the art in many natural language processing (NLP) tasks in the 1990's and up into the 2000's \cite[Sec. 22.5]{russell2016ai}.
As with most other fields of AI, the emergence of performant deep learning methods disrupted NLP in the 2010's, popularizing the methodology of using deep architectures, generally reccurent neural networks or transformers, as general purpose language models that can be fine tuned to a number of different tasks \cite{otter18dlnlp}.
This approach rarely includes any modelling of knowledge that is not induced implicitly by the contextual representations generated during pretraining on an unannotated text corpus.

The Defense Advanced Research Projects Agency (DARPA) identify the future wave of AI as one of \emph{contextual adaption}; a combination of deep learning for perception and latent representations and symbolic modelling methods \cite{darpa17ai}.
In NLP, knowledge-enhancing deep neural networks is not just an idea for the future; several approaches using explicit knowledge to improve pretrained contextual word representations (CRW's) have been presented in recent years, a subset of which will be summarized here.

\subsubsection{Static, separate knowledge graphs}
A direct way to use explicit knowledge is to maintain a separate representation of facts which can be incorporated into both pretraining and inference of deep language models.
This representation is often called a knowledge base (KB) or knowledge graph (KG) when the relational facts or entities are modelled.

In 2019, \emph{Enhanced Language Representation with Informative Entities}, ERNIE, was introduced by a Beijing team \cite{zhang2019ernie} looking to ''enhance language representation  with  external  knowledge'' \cite[1]{zhang2019ernie} by recognizing mentions of named entities in given text and retrieving their positions in a separate knowledge graph (KG).
The query from knowledge graph, explicit model of facts, is encoded into knowledge embeddings and are taken as input for a BERT-based dual transformer architecture.
The model was during training required to fill in randmoly masked named entities in given sequences using the KG.
ERNIE was not evaluated on named entity recognition in the original paper, but was evaluated on the \emph{General Language Understanding Evaluation} benchmark \cite{wang2018glue} where it did not outperform BERT.

Later in 2019, a similar idea was proposed in \emph{Knowledge Enhanced Contextual Word Representations} where the model \emph{KnowBert} \cite{peters2019knowbert} was shown to outperform ERNIE and BERT in knowledge related tasks, though GLUE or NER was not tested.
Here, the key addition was the Knowledge Attention and Recontextualization (KAR) component which allows information to retrieved from multiple KG's of different forms such as WordNET and Wikipedia and represented within the BERT encoder.

In KnowBert, though the KAR weights were trained, the KG was only used for encoding a subset of fixed facts for the model.
The \emph{Knowledge Graph Language Model} (KGLM) presented in \emph{Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling} \cite{logan2019barack} employs a more dynamic use of the knowledge graph.
The model uses the same approach to representing knowledge graph facts as ERNIE, but also builds a local graph on the sequence level which is grown by a generative model with each new token.
The paper presents improved fact completion compared to GPT-2 and further analysis has shown that the overall fact completion of KGLM is comparative to KnowBert, though they perform very differently from domain to domain, with KGLM being most dependant on the used knowledge graph \cite{birk2020knowledge}.

\subsubsection{Pretraining augmentations}
A strength of the statistical, deep approach that implicitly models knowledge is the potential for generalizable results:
You do not need a new domain specific KG for every application if enough general knowledge has been caught in your weights.
Motivated by the hope of this elusive generalizability, a number of methods have been proposed to enhance specifically the pretraining of language models to better have handle factual knowledge.
LUKE, the subject of this project, is one of these attempts.

An example of this weaker knowledge modelling is found in the 2019 model \emph{KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation} \cite{wang2021kepler} where downstream inference can be performed without any additional resources than the transformer structure itself.
The knowledge enhancement of this BERT-based language model is, however, in the pretraining where the model optimizes over a joint task of both performing masked language modelling (MLM) and a novel knowledge graph objective requiring the model to learn knowledge embeddings.
KEPLER does not improve GLUE performance from RoBERTa \cite{liu2019roberta} from which it was initialized, but beats both classical language models and knowledge enhanced ones such as ERNIE and KnowBERT on a number of knowledge related tasks.

This idea of pretraining a transformer both for MLM and for a new knowledge-guided task has been taken up multiple times, including in LUKE.
\emph{WKLM}, presented in 2019 in \emph{Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model} \cite{xiong2019wklm}, uses the English Wikipedia, taking hyperlinks to be entities, and trains an unaltered BERT architecture jointly on MLM and on a task involving identifying the randomly replaced entity in a sequence.
\emph{KALM}, published after LUKE in \emph{Knowledge-Aware Language Model Pretraining}, also keeps the core language model architecture, in this case GPT-2, unchanged, adding a separate entity tokenizer and entity embedding layer for an entity prediction pretraining task.

Though these models achieve good performance, with WKLM surpassing BERT and ERNIE on some question answering and entity related benchmarks, and KALM outperforming GPT-2 on some knowledge related zero-shot tasks, the 2020 article \emph{K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters} \cite{wang2020kadapter} raises a concern with the approach.
According to the authors, the idea of knowledge-augmenting the pretraining task and then updating the entire model limits the ability to inject versatile knowledge and sets up a heterogeneous learning problem resulting in entangled parameters that are hard to investigate.
The model \emph{K-Adapter} instead uses ''adapters'', add-ons for the transformer structure, which are pretrained for different knowledge tasks while the base RoBERTa parameters are not updated.
The resulting model outperforms BERT, WKLM, ERNIE, KEPLER and KnowBERT on central knowledge related tasks.

Another issue, raised more recently, with this jungle of knowledge enhanced models is that they do not unify generative and discriminative tasks, limiting their generalizability to fewer downstream tasks.
This is voiced in \emph{KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning} \cite{he2020kgplm} where the model KgPLM, pretrained by both generating masked entities and discriminating replaced ones, is proposed as a solution.

Which of the knowledge-enhanced models result in the best general performance for practitioners is beyond the scope of this project, as the continuous improvement of many of the models and large amount of different benchmarks used in the articles makes direct comparisons of general language understanding ability challenging.
On all benchmarks tested by the LUKE team \cite{yamada2020luke}, none of the above models have presented higher scores than LUKE \cite[Sec. 4]{yamada2020luke}.

\subsection{Deep NLP in Danish}
% Mål i afsnittet:
% daBERT skal introduceres
%
% danlp skal introduceres
%
% spacy/dacy skal introduceres
%
% Ælæktra

\section{The Report}%
\label{sec:The Report}

\end{document}
