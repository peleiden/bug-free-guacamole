\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Data}
\lhead{Data}

\section{Entity-annotated Danish Wikipedia}
For building the entity-annotated dataset, the dump of Danish Wikipedia from March 1st was used.
It consists of 80,187,419 words spanning 265,196 content pages, each of which makes up an entity in the entity vocabulary.
Due to the limited size of Danish Wikipedia, the limit on 500,000 entities from the English LUKE is not relevant.

A page consists is essentiality a sequence of words, some of which are part of hyperlinks.
These hyperlinks are treated as entity annotations, such that most articles contain a number of entity annotations.
Two special entities are added: \texttt{[MASK]} and \texttt{[UNK]}.
During training, 15\pro\ of entities are masked at random by replacing them with \texttt{[MASK]}.
%TODO: Dynamically?
\texttt{[UNK]} is used for hyperlinks that point to entities not in the vocabulary.
%TODO Consider why, and check if ever used in Danish: In English, the vocab is larger, so usage is obvious, but does it also include red links?

Every page is split into batches consisting of no more than 512 consecutive tokens and the contained hyperlinks with their entity annotations.
The set of all such sequences over all pages makes up the pretraining dataset.

\section{Named Entity Recognition}

\subsection{LUKE}
LUKE (Language Understanding with Knowledge-based Embeddings) is a language model introduced by Yamada et al. in November of 2020. \cite{yamada2020luke}.

The model produces contextualized representations of both words and entities which can then be used for fine-tuning it for specific tasks.
This is unlike other models such as BERT \cite{devlin2019bert} and RoBERTa \cite{liu2019roberta} which produce only contextualized word representations (CWR).
This allows fine-tuned tasks using LUKE to use the knowledge gained from entities, ideally improving results.

\paragraph{Pretraining}
BERT, RoBERTa, and similar models use only the token embeddings inferred directly from the given sequences when pretraining.
LUKE, which uses RoBERTa's tokenizer, also receives this.
However, the entities that Wikipedia provides as hyperlinks, are also embedded and fed to the model.

%TODO: Output representations
%TODO: Combination of LM and entity embeddings
%TODO: Pretraining
%TODO: Posttraining
%TODO: Results here?
%TODO: Entity-aware self-attention

%TODO:
\subsection{Danish NER testing sets}
\label{subsec:daNERdata}
For the task of evaluating existing Danish NER models, three publicly available datasets previously used in the literature were identified.

\paragraph{Danish Universal Dependencies}
Danish Universal Dependencies of Danish Dependency Treebank (UD-DDT) is a grammatically annotated dataset produced by converting the Danish Dependency Treebank corpus \cite{kromann2003ddt} to the Universal Dependency annotation format \cite{johann2015udddt}.
The dataset consists of 5512 sentences and 100,733 tokens from the Danish PAROLE corpus containing book, newspaper and journal literature from 1983-1992 \cite{christensen1998parole}.
UD-DDT is split into training, development and test sets consisting of 4383, 564 and 565 sentences, respectively.
Two Named Entity Recognition (NER) annotations of this dataset are considered.
\begin{itemize}
    \item \emph{DaNE} is a 2020 NER annotation of the entire UD-DDT into four categories PER(son), ORG(anisation), LOC(ation), MISC(ellaneous) performed once by a linguist and once by non-linguists for the Alexandra Institute \cite[Sec. 4]{hvingelby2020dane}
    \item A NER annotation of the development and test subsets of UD-DDT into the same four entity categories was performed by Barbara Plank in 2019 \cite{plank2019neural}.
    A subset of the training data was also annotated.
    This annotation will be called \emph{Plank}.
\end{itemize}

\paragraph{Wikipedia Annotations}
Pan et al. performed an automatic NER annotation of a Wikipedia corpus in 2017 by transferring English NER of the categories PER, ORG, LOC to 281 languages including Danish \cite{pan2017wikiann}.
This dataset is known as \emph{Wiki-ANN} (Wikipedia Annotations) and the Danish version includes 40000 sentences.
Pan et al. consider this a "silver-standard" annotation due to its automatic generation which they label Knowledge Base mining \cite[1946]{pan2017wikiann}.
The balanced training, development and test splits from Rahimi et al. are used \cite{rahimi2019transfer}.

\paragraph{Summary}
The test sets of these three NER-annotated datasets were used and a summary of their counts is shown in Table~\ref{tab:daNERdata}.
DaNE is considered the main public, Danish NER dataset as it is produced with the highest degree of expert supervision and used for training most of the models compared in Section~\ref{sec:exidan}.
Previous literature has highlighted the scarcity of accessible, gold-standard, Danish NER as a limitation in the field \cite[Sec. 2.1]{plank2019neural}.
\begin{table}[H]
    \centering
    \begin{tabular}{l|rrr|rrrr}
        \multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{Number of sentences} & \multicolumn{4}{c}{Test set entities}\\
                                &Train. & Dev. & Test &LOC & PER & ORG & MISC \\\hline
        DaNE        & 4383 & 564 & 565 & 101 & 318 & 221 & 159 \\
        Plank       & 604  & 564 & 565 & 98  & 309 & 114 & 52 \\
        Wiki-ANN (da.)   & $20\ctp3$ & $10\ctp3$ & $10\ctp3$ & 7458 & 10,660 & 10,876 & 0
    \end{tabular}
    \caption{
        Counts of sentences and test set entities in the three Danish NER benchmarks.
        Note the clear differences in annotations between DaNE and Plank, which use the same text corpus.
    }
    \label{tab:daNERdata}
\end{table}

\subsection{CoNLL-2003}
The shared task for the 2003 Conference on Computational Natural Language Leanring (CoNLL) was language-independent NER for which annotations on English Reuters news wire articles were performed by researchers \cite{tjang2003conll}.
This corpus consists of 1393 articles that were divided into training, development and test sets as seen in Table~\ref{tab:conll2003}.
The dataset is the central benchmark in the field of Named Entity Recognition \cite[Sec. 4.3]{yamada2020luke} and has a competitive history with over 50 models submitted to the performance leaderboard at Papers With Code\footnotemark.
\footnotetext{
    See 
    \url{
        https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003
    }.
    As of 26/02, 2021, LUKE tops the leaderboard.
}
\begin{table}[H]
    \centering
    \begin{tabular}{l|ccc|cccc}
    CoNLL-2003 English  & Articles  & Sentences  & Tokens   & LOC   & PER   & ORG   & MISC  \\ \hline
    Training set        & 946       & 14,987     & 203,621  & 7140  & 6600  & 6321  & 3438   \\
    Development set     & 216       & 3466      & 51,362   & 1837  & 1842  & 1341  & 922    \\
    Test set            & 231       & 3684      & 46,435   & 1668  & 1617  & 1661  & 702    \\
    \end{tabular}
    \caption{
        Dataset counts for the canonical NER dataset CoNLL-2003.
        Note the more even distribution of entities when compared to the Danish NER datasets shown at Table~\ref{tab:daNERdata}, highlighting the difference in entities between short news texts and more general-purpose literature.
    }
    \label{tab:conll2003}
\end{table}

\subsection{Annotation schemes: What do the tags mean?}
The DaNE and Plank sets refer to the CoNLL-2003 as their reference annotation scheme \cite[Sec. 4]{hvingelby2020dane} \cite[Sec. 2.1]{plank2019neural}.
Wiki-ANN follows the same categories, excluding MISC.
All datasets follow the Inside-outside-beginning (IOB) format that ascribes a word the "I-X" tag if it is inside a named entity of type X, "B-X" if it begins the X entity and "O" if it is outside named entities (e.g. the null class).
This format proposed by Ramshaw and Marcus in 1995 is widely used but does allow entities to be nested or to overlap \cite{ramshaw1995IOB}.

The definition of the annotation categories are given in the CoNLL-2003 guidelines \footnotemark and are summarized here:
\footnotetext{
    These are available here: \url{https://www.clips.uantwerpen.be/conll2003/ner/annotation.txt} (collected 27/02-2021)
}
\begin{itemize}
    \item LOC: Geographical regions, natural locations and public and commercial places. Also includes abstract places.
    \item PER: Names, aliases of people, animals and fictional characters.
    \item ORG: Companies, brands, government bodies, movements, clubs and subdivisions thereof.
    \item MISC: Titles of media, nationalities, languages, religions, ideologies, wars, slogans, eras, but also both adjectives and word combinations that are derived from one of the other tags.
\end{itemize}
To end the introduction of the data, some examples of named entities in each testing dataset are shown.
Note, that the all-important context of the words is not shown here.
\begin{itemize}
    \item DaNE: helvede (LOC), Holland (LOC), Astrid Lindgren (PER), Odense Teater (ORG), det danske rigsfællesskab (ORG), afghansk (MISC), Camel (MISC)
    \item Plank: Bagdad (LOC), Bjarne (PER), USA (ORG), Københavns Kommune (ORG), DR-dokumentar (MISC), Levi's jeans (MISC)
    \item Wiki-ANN (da.): Dinariske Apler (LOC), Esbjerg Kommune (LOC), Jorge Luis Borges (PER), FC Barcelona (ORG), kommunehospitals (ORG)
    \item CoNLL-2003: Buenos Aires (LOC), Whistler Mountain (LOC), Katja Seizinger (PER),  FIFA (ORG), Asian Cup (MISC), Swede (MISC)
\end{itemize}





\end{document}
