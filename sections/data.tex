% !TeX spellcheck = en_GB
\documentclass[main.tex]{subfiles}

\begin{document}
\lhead{Theory and State of the Art}
\chapter{Data}
\lhead{Data}
\label{chap:data}

\section{Entity-annotated Danish Wikipedia}
\label{sec:dawiki}
For the pretraining of LUKE, a corpus of entity-annotated text is needed.
Entity annotations consist of two things: The word span and the name of the entity.
No text is written directly with such annotations in mind.
However, some texts such as news articles often contain embedded hyperlinks that implicitly provide the word spans, but these often point to other articles rather than specific items.
On Wikipedia, this is not the case, as hyperlinks point to articles on specific topics, which makes it suitable for LUKE's pretraining task.
Given its size, Wikipedia is thus a particularly useful pretraining dataset.

As of June 23rd, the Danish Wikipedia has 267,408 content pages containing a total of 81,822,460 words\footnote{\url{https://da.wikipedia.org/wiki/Speciel:Statistik}. Visited June 23, 2021.}.
The Danish Wikipedia from March 1st, 2021, which is marginally smaller, was used for building the DaLUKE dataset.
Yamada et al. limit the entity vocabulary to the 500,000 most often linked pages \cite{yamada2020luke}.
As the Danish Wikipedia is only roughly 2.5\pro\ the size of the English Wikipedia, no limit on the entity vocabulary was imposed, to preserve data.

Every article is a sequence of words with embedded hyperlinks.
These were split into sequences of at most 512 tokens using BotXO's Danish BERT tokenizer, and the token spans of the hyperlinks were saved.
Following BERT \cite{devlin2019bert}, the first and last tokens were always [CLS] and [SEP], respectively, with the remaining 510 tokens coming directly from the tokenizer.
Sequences were cut short if necessary to prevent hyperlinks spanning two sequences.
All sequences contain text from at most one paragraph in an article.

Word spans over tokens were also saved, as the MLM pretraining task was done on a full word level.
Subword and entity tokens were masked using the [MASK] token.
The [UNK] special token is reserved for subwords that are not part of the tokenizer's vocabulary.

%TODO Make sure itemize is not broken up with a page
Thus, an entry in the dataset -- called an example -- consists of the following:
\begin{itemize}
    \item An array of at most 512 subwords token ID's
    \item An array of at most 128 entity ID's
    \item An array of two-tuples containing the start and end indices for every entity
    \item An array of two-tuples containing the start and end indices for every full word
\end{itemize}
Finally, the pretraining dataset was the set of all examples from all articles.

\subsection{Entity Augmentations}
\label{subsec:entaug}
One limitation of Wikipedia as a pretraining dataset is that hyperlinks are rarely repeated, even if the same article is referenced multiple times on a page.
This results in false negatives -- cases where entities are not annotated as such.

In order to augment the entity annotations, we performed preprocessing on Wikipedia before pretraining.
Our preprocessing consisted of the following two steps applied article-wise:
\begin{enumerate}
    \item Collect the set of mentioned articles including the title of the article itself.
    Only include titles of at least three characters.
    \item For each title, check if it matches a sequence of characters following a whitespace while ignoring casing.
    If there is a match, annotate and go to next whitespace.
\end{enumerate}
Many hyperlinks span only the first part of a given word.
For instance, this allows the word "Windmills" to link to the article "Windmill".
When matching, such cases were annotated as we expected it to remove numerous false negatives, while introducing relatively few false positives.
Not annotating titles shorter than three characters prevented the article on "A" to be linked at every word starting with "A".
We also only matched titles already linked in the articles, as we assumed that any article that should be linked to, had already been linked at least once.
This should further reduce the number of false positives.
Whether this had a positive effect on learning, is subject to an experiment presented in Section~\ref{subsec:dataexp}.

Table \ref{tab:metadata} shows how these augmentation steps change the properties of the dataset.
\begin{table}[H]
    \centering
    \begin{tabular}{l|r|r|r}
        Statistic&Unaugmented	&Augmented    &Difference	\\\hline
        Subword tokens&101,394,208	&108,322,681 &+6,8\pro	\\
        Entity vocabulary size&214,150  &225,198    &+5,2\pro   \\
        Entity annotations&4,883,918	&7,180,372 &+47,0\pro\\
        Examples&363,498   &378,357 &+4,1\pro
    \end{tabular}
    \caption{
        Statistics for both datasets.
        The increase in entity vocabulary size is explained by adding links that point to the article in which the link occurs.
        This means that some articles, which are never linked to, have links in the augmented dataset, causing to be included in the entity vocabulary.
        The increased number of subword tokens and examples comes from not including sentences without entities, of which there are fewer in the augmented dataset.
    }
    \label{tab:metadata}
\end{table}\noindent


\section{Named Entity Recognition Benchmarks}
\label{sec:nerdata}

\subsection{Danish Named Entity Recognition Datasets}
\label{subsec:daNERdata}
For the task of benchmarking both existing and new Danish NER models, three publicly available datasets previously used in the literature were identified.

\paragraph{Danish Universal Dependencies}
Universal Dependencies of Danish Dependency Treebank (UD-DDT) is a grammatically annotated dataset produced by converting the Danish Dependency Treebank corpus \cite{kromann2003ddt} to the Universal Dependency annotation format \cite{johann2015udddt}.
The dataset consists of 5512 sentences and 100,733 characters from the Danish PAROLE corpus containing book, newspaper and journal literature from 1983-1992 \cite{christensen1998parole}.
UD-DDT is split into training, development and test sets consisting of 4383, 564 and 565 sentences, respectively.
Two NER annotations of this dataset are considered.
\begin{itemize}
    \item \emph{DaNE} is a 2020 NER annotation of the entire UD-DDT into four categories PER(son), ORG(anisation), LOC(ation), MISC(ellaneous) performed once by a linguist and once by a team of non-linguists for the Alexandra Institute \cite[Sec. 4]{hvingelby2020dane}.
    The categories are described in closer detail in Section \ref{subsec:annoschemes}.
    \item A NER annotation of the development and test subsets of UD-DDT into the same four entity categories was performed by Barbara Plank in 2019 \cite{plank2019neural}.
    A subset of the training data was also annotated.
    This annotation will be called \emph{Plank}.
\end{itemize}

\paragraph{Wikipedia Annotations}
Pan et al. performed an automatic NER annotation of a Wikipedia corpus in 2017 by transferring English NER of the categories PER, ORG, and LOC to 281 languages including Danish \cite{pan2017wikiann}.
This dataset is known as \emph{WikiANN} (Wikipedia Annotations) and the Danish version includes 40,000 sentences.
Pan et al. consider this a "silver-standard annotation" due to its automatic generation which they label knowledge base mining \cite[1946]{pan2017wikiann}.
The balanced training, development, and test splits from Rahimi et al. are used \cite{rahimi2019transfer}.

\paragraph{Comparison}
These three NER-annotated datasets were used and a summary of their counts is shown in Table~\ref{tab:daNERdata}.
DaNE is considered the main public Danish NER dataset, as it is produced with the highest degree of expert supervision and used for training most of the models compared in Section~\ref{sec:exidan}.
However, as shown on Table \ref{tab:danedist} and Figure \ref{fig:dadatadist}, DaNE and Plank suffer from uneven label distributions in the different subsets.
As most model training assumes that the training data has the same distribution as the true distribution, this could cause evaluations on the test sets to give a poor approximation of real-world performance.
Previous literature has highlighted the scarcity of accessible, gold-standard, Danish NER data as a limitation in the field \cite[Sec. 2.1]{plank2019neural}.
\begin{table}[H]
    \centering
    \begin{tabular}{l|rrr|rrrr}
        \multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{Number of sentences} & \multicolumn{4}{c}{Test set entities}\\
                            &\jl{Train} & \jl{Dev.} & \jl{Test} &\multicolumn{1}{|l}{LOC} & \jl{PER} & \jl{ORG} & \jl{MISC} \\\hline
        DaNE                & 4,383     & 564       & 565       & 96    & 180    & 161    & 121 \\
        Plank               & 604       & 564       & 565       & 97    & 169    & 94     & 30 \\
        WikiANN (da.)       & $20\ctp3$ & $10\ctp3$ & $10\ctp3$ & 5,242 & 4,378  & 4,078  & 0
    \end{tabular}
    \caption{
        Counts of sentences and test set entities in the three Danish NER benchmarks.
        Note the clear differences in annotations between DaNE and Plank, which use the same text corpus.
    }
    \label{tab:daNERdata}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{l|r r r r}
        Subset of data  &LOC    &PER     &ORG    &MISC \\\hline
        Training        &945    &1,249   &802    &1,007 \\
        Dev.            &111    &166     &90     &113 \\
        Test            &96     &180     &161    &121
    \end{tabular}
    \caption{The number of NEs by category and data split in DaNE.}
    \label{tab:danedist}
\end{table}\noindent

\begin{figure}[H]
    \centering
   	\begin{minipage}[t]{0.32\textwidth}
       \includegraphics[width=\textwidth]{datadist_dane}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{datadist_plank}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.32\textwidth}
        \includegraphics[width=\textwidth]{datadist_wikiann}
    \end{minipage}
    \caption{
        Distribution of entity labels in the three Danish NER datasets.
        Notably, DaNE and Plank do not have the same distributions in the three datasets, with e.g. LOC being under-represented in the DaNE test set and MISC being over-represented in the Plank test set.
        WikiANN is the only dataset that has the same distribution in all three subsets.
    }
    \label{fig:dadatadist}
\end{figure}\noindent

\subsection{CoNLL-2003}
The shared task for the 2003 Conference on Computational Natural Language Learning (CoNLL) was NER for which annotations on English Reuters news wire articles were performed by researchers \cite{tjang2003conll}.
This corpus consists of 1393 articles that were divided into training, development and test sets as seen in Table~\ref{tab:conll2003}.
The dataset is the central benchmark in English NER \cite[Sec. 4.3]{yamada2020luke} and has a competitive history with over 50 models submitted to the performance leaderboard at Papers With Code \footnotemark.
\footnotetext{
    See 
    \url{
        https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003
    }.
    As of June 2021, LUKE is second in the leaderboard.
}
\begin{table}[H]
    \centering
    \begin{tabular}{l|rrr|rrrr}
    CoNLL-2003 English  & Articles  & Sentences  & Tokens   & LOC   & PER   & ORG   & MISC  \\ \hline
    Training set        & 946       & 14,987     & 203,621  & 7,140  & 6,600  & 6,321  & 3,438   \\
    Development set     & 216       & 3,466      & 51,362   & 1,837  & 1,842  & 1,341  & 922    \\
    Test set            & 231       & 3,684      & 46,435   & 1,668  & 1,617  & 1,661  & 702    \\
    \end{tabular}
    \caption{
        Dataset counts for the canonical NER dataset CoNLL-2003.
        We note multiple differences in NE distributions compared to the Danish NER datasets shown at Table~\ref{tab:daNERdata} and take this as an indication of the different corpus backgrounds.
    }
    \label{tab:conll2003}
\end{table}

\subsection{Annotation Schemes: What Do The Tags Mean?}
\label{subsec:annoschemes}
The DaNE and Plank sets refer to the CoNLL-2003 as their reference annotation scheme \cite[Sec. 4]{hvingelby2020dane} \cite[Sec. 2.1]{plank2019neural}.
WikiANN follows the same categories, excluding MISC.
All datasets follow the Inside-outside-beginning (IOB) format that ascribes a word the "I-X" tag if it is inside a named entity of type X, "B-X" if it begins the entity, and "O" if it is outside named entities (e.g. the null class).
This format proposed by Ramshaw and Marcus in 1995 is widely used but does allow entities to be nested or to overlap \cite{ramshaw1995IOB}.

\begin{table}[H]
    \centering
    \begin{tabular}{l|llllll}
        Text example      & Antonio & Vivaldi & came & from & Venice\\\hline
        IOB annotation    & B-PER   & I-PER   &  O   & O    & B-LOC   \\
    \end{tabular}
\end{table}\noindent
The definition of the annotation categories are given in the CoNLL-2003 guidelines\footnotemark~and are summarized here:
    \footnotetext{
        These are available at \url{clips.uantwerpen.be/conll2003/ner/annotation.txt}. Visited February 27, 2021.
}
\begin{itemize}
    \item LOC: Geographical regions, natural locations, and public and commercial places. Also includes abstract places.
    \item PER: Names, aliases of people, animals, and fictional characters.
    \item ORG: Companies, brands, government bodies, movements, clubs, and subdivisions thereof.
    \item MISC: Titles of media, nationalities, languages, religions, ideologies, wars, slogans, eras, and adjectives and word combinations that are derived from one of the other tags.
\end{itemize}
To end the introduction of the data, some examples of NEs in each testing dataset are shown.
Note that the all-important context of the words is not shown here.
\begin{itemize}
    \item DaNE: helvede (LOC), Holland (LOC), Astrid Lindgren (PER), Odense Teater (ORG), det danske rigsfællesskab (ORG), afghansk (MISC), Camel (MISC)
    \item Plank: Bagdad (LOC), Bjarne (PER), USA (ORG), Københavns Kommune (ORG), DR-dokumentar (MISC), Levi's jeans (MISC)
    \item WikiANN (da.): Dinariske Alper (LOC), Esbjerg Kommune (LOC), Jorge Luis Borges (PER), FC Barcelona (ORG), kommunehospitals (ORG)
    \item CoNLL-2003: Buenos Aires (LOC), Whistler Mountain (LOC), Katja Seizinger (PER),  FIFA (ORG), Asian Cup (MISC), Swede (MISC)
\end{itemize}

\end{document}
