% !TeX spellcheck = en_GB
\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Conclusion}
\lhead{Conclusion}
We present DaLUKE, a Danish version of the English LUKE \cite{yamada2020luke}, a general-purpose language model for producing contextualized word and entity representations.
It is fine-tuned on a number of Danish named entity recognition (NER) datasets and achieves close to state of the art on DaNE, the primary Danish NER dataset, being slightly behind DaCy large \cite{enevoldsen2020dacy}.
A large number of Danish named entity recognition results are reproduced.
Notably, DaLUKE beats DaNLP's fine-tuned version of BotXO's Danish BERT \cite{danlp2021, botxo2019dabert}, indicating that the knowledge-based additions of Yamada et al. \cite{yamada2020luke} do indeed raise the performance of the model.
Furthermore, the both released LUKE models (large and base) are fine-tuned on the CoNLL2003 NER dataset \cite{tjang2003conll}, and the results reported by Yamada et al. are reproduced.

Both the general-purpose pretrained and the fine-tuned DaLUKE are made publicly available under the open source MIT license \cite{mitlicense} along with a \code{pip} installable package, \code{daluke}, that allows for easy use and integration into existing code bases.

Ablation studies are conducted to investigate the effects on the pretraining of the applied data engineering consisting of adding more entities to the dataset, Yamada et al.'s proposed entity-aware self-attention, reducing the entity vocabulary \cite{yamada2020luke}, and transfer learning by initializing much of the model weights to those of Danish BERT.
Using DaNE as a benchmark, entity-aware self-attention and a full entity vocabulary are found to help learning, and that especially transfer learning is important to the performance of the model.
Without transfer learning, the model seems to overfit on the pretraining dataset, which is a concern with small datasets as risks being the case on low-resource languages like Danish.
Our data-engineering, however, results in slightly worse performance (though within margin of error), highlighting how changes in the pretraining dataset can have unintuitive consequences and should be applied carefully.

The fine-tuning stability is also explored; we find it to sensitive to hyperparameters and especially the random number generation seed -- enough to where the seed may influence what model actually performs the best on DaNE.

All in all, DaLUKE shows good results but has several paths that are worth exploring to further improve performance.

\end{document}
