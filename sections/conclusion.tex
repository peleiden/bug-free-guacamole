% !TeX spellcheck = en_GB
\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Conclusion}
\lhead{Conclusion}
We present DaLUKE, a Danish version of the English LUKE \cite{yamada2020luke}, a general-purpose language model for producing contextualized word and entity representations.
It is fine-tuned on a number of Danish named entity recognition (NER) datasets and achieves close to state of the art on DaNE, the primary Danish NER dataset, being slightly behind DaCy large \cite{enevoldsen2020dacy}.
A large number of Danish named entity recognition results are reproduced.
Notably, DaLUKE beats DaNLP's fine-tuned version of BotXO's Danish BERT \cite{danlp2021, botxo2019dabert}, indicating that the knowledge-based additions of Yamada et al. \cite{yamada2020luke} do indeed raise the performance of the model.
Furthermore, the both released LUKE models (large and base) are fine-tuned on the CoNLL2003 NER dataset \cite{tjang2003conll}, and the results reported by Yamada et al. are reproduced.

Both the general-purpose pretrained and the fine-tuned DaLUKE are made publicly available under the open source MIT license \cite{mitlicense} along with a \code{pip} installable package, \code{daluke}, that allows for easy use and integration into existing code bases.

A number of ablation studies are conducted to investigate the effects on the pretraining of both the applied data engineering consisting of adding more entities to the dataset, Yamada et al.'s proposed entity-aware self-attention, reducing the entity vocabulary \cite{yamada2020luke}, and the effects of transfer learning by initializing much of the model weights to those of Danish BERT.
Using DaNE as a benchmark, we find that entity-aware self-attention and a full entity vocabulary helps learning, and that especially transfer learning is important to the performance of the model.
Without transfer learning, the model seems to overfit on the pretraining dataset, which is always a concern with small datasets as risks being the case on low-resource language such as Danish.
Our data-engineering, however, results in slightly worse performance (though within margin of error), highlighting how changes in the pretraining dataset can have unintuitive consequences and should be applied carefully.

The stability of the fine-tuning is also explored; we find it to be highly sensitive to hyperparameters unlike the pretraining, and even the seed used for random number generation plays a large role in the results -- enough to where it may influence what model actually performs the best on DaNE.

\end{document}
