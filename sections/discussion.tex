\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Experiments and Discussion}
\lhead{Experiments and Discussion}

\section{What Is Going on in the Pretraining?}
\label{sec:pretrainpls}
In order to investigate what affects pretraining effectiveness, several ablation studies are performed.
Unless stated otherwise, the hyperparameters from table \ref{tab:pretrain-hyper} are used, but with 50 epochs due to limited compute.
The experiments were trained primarily on a $ 1\times$A100 configuration.
Due to its negative effect on runtime on A100's (see figure \ref{fig:runtime}), AMP is not used for any of the following experiments.
%TODO Overvej, om introspection skal stå her. Ellers skal ovenstående formuleringer ændres
%TODO Beskriv fine-tuning: Bare en enkelt finetuning eller flere for at korrigere for varians? Nævn hyperparametre, evt med tabelhenvisning
%TODO Tal om den meget store initialiseringsvarians - måske ikke så vigtig? De indhenter jo det alle sammen
%TODO Lukas' ting om da-BERTs undertræning. Måske et andet sted end her

All of the models presented here are evaluated on the DaNE test set \emph{with} MISC annotations included.
This is because only the performances relative to the baseline model rather than other NER models is interesting.

\subsection{Introspection: The Parameter Population}

\subsection{Baseline}
For a baseline model, the main model is retrained using only 50 epochs, but otherwise using the same hyperparameters, shown in Table \ref{tab:pretrain-hyper}.
This allows for a comparison to a number of ablation studies where compute resources did not allow full 150 epoch pretraining.

The final results of this pretraining are summarized in table \ref{tab:baseline-mlm} with the accuracy development shown on figure \ref{fig:baseline-acc}.

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|cccccc}
        Model                           & Top $k$ accuracy [\pro]  & $k=1$  & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$\\\hline
        \multirow{2}{*}{Baseline}       & Masked words             & 24.23  & 30.94 & 34.37 & 39.70  & 47.71  & 53.62 \\
                                        & Masked entities          & 28.80  & 39.01 & 50.64 & 50.64  & 59.57  & 66.32
    \end{tabular}
    \caption{
        The top $k$ accuracy of the baseline model in the 50'th and last epoch.
    }
    \label{tab:baseline-mlm}
\end{table}\noindent
% 2021-06-11 19:37:14.230    INFO        K=1
%                                        Word:   24.231
%                                        Entity: 28.800
% 2021-06-11 19:37:14.235    INFO        K=3
%                                        Word:   30.937
%                                        Entity: 39.009
% 2021-06-11 19:37:14.238    INFO        K=5
%                                        Word:   34.371
%                                        Entity: 43.843
% 2021-06-11 19:37:14.242    INFO        K=10
%                                        Word:   39.701
%                                        Entity: 50.637
% 2021-06-11 19:37:14.245    INFO        K=25
%                                        Word:   47.708
%                                        Entity: 59.567
% 2021-06-11 19:37:14.249    INFO        K=50
%                                        Word:   53.619
%                                        Entity: 66.324
\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth]{baseline-acc}
    \caption{Baseline masked language and masked entity accuracy throughout pretraining.}
    \label{fig:baseline-acc}
\end{figure}\noindent
The model is afterwards finetuned for named entity recognition on DaNE following the approach described in Section~\ref{sub:finetune-ner} and the following hyperparameters, non-systematically selected:
\begin{table}[H]
    \centering
    \begin{tabular}{l|r}
        Parameter  &    \jl{Value}\\\hline
        Epochs     & 10\\
        Batch size &    16\\
        Peak learning rate & $2\ctp{-5}$\\
        LR warmup steps proportion & $ 6\pro $\\
        Dropout (pretrained model) & $ 0.1 $\\
        Dropout (final, linear layer) & $ 0.025 $\\
        Weight decay & $ 0.05 $\\
        Loss weighting & No\\
    \end{tabular}
    \caption{Hyperparameters for baseline finetuning experiment and the following ablation experiments.}\label{tab:baseline-hyper}
\end{table}\noindent
This resulted in the following performance on the DaNE test data-set.
\begin{table}[H]
    \centering
    \begin{tabular}{l|ccccc|c|c}
        \multirow{2}{*}{Model}  & \multicolumn{5}{c|}{F1 [\pro]} & Precision [\pro]               & Recall [\pro]               \\
                            & Avg. & LOC & PER & ORG & MISC      & Avg.                           & Avg.                         \\ \hline
    Baseline                & 82.41&89.58&92.90&74.48&68.84      & 86.73                          & 78.49
    \end{tabular}
    \caption{The baseline finetuning results where avg. refers to the micro average}
    \label{tab:summary}
\end{table}\noindent
The results of the baseline experiment together with the following experiments are also shown together in a final overview at Section \ref{subsec:preoverview}.
 % 2021-06-11 19:53:46.665    INFO                      precision    recall f1-score   support

 %                                                LOC     0.8958    0.8958   0.8958        96
 %                                               MISC     0.7872    0.6116   0.6884       121
 %                                                ORG     0.8372    0.6708   0.7448       161
 %                                                PER     0.9140    0.9444   0.9290       180

 %                                          micro avg     0.8673    0.7849   0.8241       558
 %                                          macro avg     0.8586    0.7807   0.8145       558
 %                                       weighted avg     0.8612    0.7849   0.8180       558

\subsection{Dataset Augmentation}

\ref{tab:metadata}
A key addition to the pretraining pipeline was the addition of extra entity annotations not already in the Wikipedia articles themselves using pattern matching as explained in Section~\ref{subsec:entaug}.
This resulted in a 47\pro\ growth in the number of annotations as shown in Table~\ref{tab:metadata}.
While it was argued that it should improve performance, it was all theoretical.
Therefore, this effect of the augmentation is investigated by pretraining a model with the original data for 50 epochs and same hyperparameters as the main experiment.

The pretraining terminated to the following performance.

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|cccccc}
        Model                               & Top $k$-accuracy [\pro]  & $k=1$  & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$\\\hline
        \multirow{2}{*}{No data aug.}       & Masked words             & 25.26  & 32.16 & 35.60 & 40.93  & 48.78  & 54.62 \\
                                            & Masked entities          & 34.02  & 42.85 & 47.21 & 53.52  & 62.26 & 68.82
    \end{tabular}
    \caption{
        The top $k$ accuracy of the pretrained model trained without dataset augmentation in the 50'th and last epoch.
    }
    \label{tab:old-data-mlm}
\end{table}
At a first glance, these masked language modelling results that strictly dominate the numbers for the baseline seems to suggest that the dataset augmentation was ill-advised.
There is, however, a large issue (apart from the obvious problem of judging a model by it's performance on the training data set) with the metric in this case:
The dataset augmentation also changed the benchmark itself as the masking task of the baseline model also includes automatically annotated, and thus somewhat dubious, entity links.
This can explain the clear gain in entity masking performance.
Whether this explanation has any merit in explaining the increased word accuracy found in this ablation study is less clear to us and relates to the model synergy effects in this joint task.

% 2021-06-11 19:38:23.333    INFO        K=1
%                                        Word:   25.260
%                                        Entity: 34.015
% 2021-06-11 19:38:23.340    INFO        K=3
%                                        Word:   32.155
%                                        Entity: 42.846
% 2021-06-11 19:38:23.345    INFO        K=5
%                                        Word:   35.601
%                                        Entity: 47.212
% 2021-06-11 19:38:23.351    INFO        K=10
%                                        Word:   40.926
%                                        Entity: 53.520
% 2021-06-11 19:38:23.356    INFO        K=25
%                                        Word:   48.776
%                                        Entity: 62.258
% 2021-06-11 19:38:23.361    INFO        K=50
%                                        Word:   54.617
%                                        Entity: 68.818
As a more unbiased benchmark, this model was finetuned on the DaNE data set using the hyperparameters at Table~\ref{tab:baseline-hyper} resulting in the following performance:
\begin{table}[H]
    \centering
    \begin{tabular}{l|ccccc|c|c}
        \multirow{2}{*}{Model}  & \multicolumn{5}{c|}{F1 [\pro]} & Precision [\pro]               & Recall [\pro]               \\
                            & Avg. & LOC & PER & ORG & MISC      & Avg.                           & Avg.                         \\ \hline
    No data aug.            & 83.07&84.42&95.18&76.41&72.10      & 85.42                          & 80.82
    \end{tabular}
    \caption{The finetuning results of the dataset augmentation pretraining experiment.}
    \label{tab:dataaug}
\end{table}
% 2021-06-11 16:08:34.724    INFO                      precision    recall  f1-score   support

%                                                 LOC     0.8155    0.8750    0.8442        96
%                                                MISC     0.7500    0.6942    0.7210       121
%                                                 ORG     0.8214    0.7143    0.7641       161
%                                                 PER     0.9711    0.9333    0.9518       180

%                                           micro avg     0.8542    0.8082    0.8306       558
%                                           macro avg     0.8395    0.8042    0.8203       558
%                                        weighted avg     0.8532    0.8082    0.8291       558
The ablation study gets substantially higher recall than the baseline resulting in a higher micro average F1 score on the NER task, though the baseline outperforms in precision.

All in all, these benchmarks cannot be used to argue that the theorized problem of of false negatives in the volunteer-produced annotations is mitigated by this augmentation approach resulting in a better language understanding.
If anything, the addition of automatic pattern-matched annotations worsened the performance slightly.

As the addition of these $47\pro$ bronze standard annotations did directly stop the learning, we still propose this approach as an avenue for further development of DaLUKE.
Much needed entirely new LUKE pretraining data sets could be produced from raw text corpora by automatically annotating them using pattern matching.
Such lower standard annotations used to achieve extra data might be necessary for continued improvement in low-resource languages such as Danish but should be used carefully and with an analysis of the trade off between quality and quantity of data.

\subsection{Entity-aware Self-attention}
One of Yamada et al.'s key contributions to the transformer is the addition of query matrices for relations between entities and words.
They do not use these extra matrices in the pretraining but instead let them be learned during downstream tasks, initializing them to the word-to-word matrices of RoBERTa.
They perform ablation studies on multiple datasets, showing an improvement in every case over original attention.
\cite{yamada2020luke}

These results suggest that training the extra query matrices in the pretraining also might have a positive impact.
For this reason, entity-aware self-attention has been used here for all pretrainings bar this one, where its effects on pretraining is investigated.

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|cccccc}
        Model                               & Top $k$-accuracy [\pro]  & $k=1$  & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$\\\hline
        \multirow{2}{*}{BERT attention}     & Masked words             & 20.63  & 26.29 & 29.41 & 34.58  & 42.44  & 48.73 \\
                                            & Masked entities          & 12.26  & 20.98 & 25.69 & 32.76  & 42.88 & 51.52
    \end{tabular}
    \caption{
        The top $k$ accuracy of the pretrained model trained without entity-aware self-attention in the 50'th and last epoch.
    }
    \label{tab:bert-attention-mlm}
\end{table}\noindent
After finetuning, the following results were achieved:

\begin{table}[H]
    \centering
    \begin{tabular}{l|ccccc|c|c}
        \multirow{2}{*}{Model}  & \multicolumn{5}{c|}{F1 [\pro]} & Precision [\pro]               & Recall [\pro]               \\
                            & Avg. & LOC & PER & ORG & MISC      & Avg.                           & Avg.                         \\ \hline
    BERT attention          & 79.62 & 85.86 & 92.84 & 70.95 &   64.84      & 84.51                          & 75.27
    \end{tabular}
   \caption{The finetuning results of the traditional attention experiment.}
    \label{tab:bertatt}
\end{table}

% 2021-06-12 12:21:29.881    INFO                      precision    recall  f1-score   support

%                                                 LOC     0.7191    0.6667    0.6919        96
%                                                MISC     0.6585    0.4463    0.5320       121
%                                                 ORG     0.6364    0.2609    0.3700       161
%                                                 PER     0.7563    0.5000    0.6020       180

%                                           micro avg     0.7022    0.4480    0.5470       558
%                                           macro avg     0.6926    0.4685    0.5490       558
%                                        weighted avg     0.6941    0.4480    0.5354       558
\begin{enumerate}
    \item Performance og træningskurver uden entity-aware attention
%    \item Køretidssammenligning på de to modeller
\end{enumerate}

\subsection{Impact of Danish BERT}
As described in section \ref{sec:LUKE}, many of the weights are initialized from a base transformer.
This transfer step ideally gives DaLUKE all the contextual language understanding of da-BERT without having to learn it all.
In order to test this hypothesis, a pretraining is conducted without initializing the weights to da-BERT.

\begin{table}[H]
    \centering
    \begin{tabular}{l|l|cccccc}
        Model                                 & Top $k$ accuracy [\pro]  & $k=1$  & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$\\\hline
        No transfer & Masked words             & 47.86  & 59.28 & 63.92 & 69.83  & 76.51  & 81.04 \\
        learning                                      & Masked entities          & 81.71  & 87.19 & 88.87 & 90.90  & 93.15 & 94.56
    \end{tabular}
    \caption{
        The top $K$ accuracy of the pretrained model trained without initializing weights from da-BERT in the 50'th and last epoch.
    }
    \label{tab:nobert-mlm}
\end{table}\noindent
It is immediately clear from table \ref{tab:nobert-mlm} that this model performs noticably better at both the masked word and masked entity tasks than other models so far.
That is despite even starting from 0 in the masked word task (see figure \ref{fig:nobert-acc}).
Because of this, good NER performance could reasonbly be expected.
This did not happen - in fact, results were much worse than the baseline.
% 2021-06-12 12:20:54.130    INFO                      precision    recall  f1-score   support

%                                                 LOC     0.7281    0.8646    0.7905        96
%                                                MISC     0.6514    0.5868    0.6174       121
%                                                 ORG     0.7841    0.4286    0.5542       161
%                                                 PER     0.8683    0.8056    0.8357       180

%                                           micro avg     0.7699    0.6595    0.7104       558
%                                           macro avg     0.7580    0.6714    0.6995       558
%                                        weighted avg     0.7728    0.6595    0.6994       558

\begin{table}[H]
    \centering
    \begin{tabular}{l|ccccc|c|c}
        \multirow{2}{*}{Model}  & \multicolumn{5}{c|}{F1 [\pro]} & Precision [\pro]               & Recall [\pro]               \\
                            & Avg. & LOC & PER & ORG & MISC      & Avg.                           & Avg.                         \\ \hline
    No transfer learning    & 71.04&79.05&83.57&55.42&61.74      & 76.99                          & 65.95
    \end{tabular}
    \caption{The finetuning results of the experiment where weights were trained from scratch}
    \label{tab:nobert}
\end{table}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{nobert-acc.png}
    \caption{Development of the masked word and entity accuracies during the course of the pretraining without initializing from da-BERT weights.
    All curves are smoothed using a rolling average.}
    \label{fig:nobert-acc}
\end{figure}\noindent
The transfer learning from da-BERT clearly turns out to be a help here.
From figure \ref{fig:nobert-acc}, the initialization seems to provide two things other than initial MLM accuracy:
\begin{itemize}
    \item A regulazation effect that prevents the model from overfitting to the dataset.
    \item Increased pretraining stability.
    The accuracy curves contain a number of sudden jumps and drops that are much rarer in other pretrainings.
\end{itemize}
It should be noted that because no weights came from da-BERT, no weights were ever locked throughout pretraining.
A similar experiment with weights unlocked and initialization from da-BERT showed pretraining accuracies much closer to the baseline, indicating that the regualizing effect is indeed da-BERT.
%TODO Under-undereksperimentet skal måske formaliseres mere

\begin{enumerate}
    \item Performance og træningskurver for eksperimentet, hvor BERT-vægte ikke bruges
    \item Her rapporteres under-eksperimentet med performance for da-BERT på vores datasæt
    \item Her kan henvises til parameterplotsene i en diskussion af BERT-vægtenes rolle
\end{enumerate}

\subsection{Entity Vocabulary Size}
For the English LUKE, Yamada et al. used an entity vocabulary of the $500\ctp3$ entities \cite[Sec. 3.4]{yamada2020luke} even though the English Wikipedia contains $\sim 6\ctp6$ content pages\footnotemark.
\footnotetext{English Wikipedia Statistics: \url{https://en.wikipedia.org/wiki/Special:Statistics}. Visited March 3, 2021.}
This is, apart from crude filtering of non-entity pages, a result of cutting the entity domain down to the most frequent hyperlinks.
For the Danish Wikipedia, however, the number of content pages is $\sim 267\ctp3$ \footnotemark resulting in the DaLUKE entity vocabulary containing $\sim 225\ctp3$ entities.
\footnotetext{Danish Wikipedia Statistics: \url{https://da.wikipedia.org/wiki/Speciel:Statistik} Visited March 3, 2021.}
As this is a much smaller world of entities, the entity vocabulary of the main DaLUKE model was not cut by frequency.

An experiment was performed where entities that were mentioned less than 50 times were excluded from the data, resulting in a vocabulary of $19,119$ entities.
This was done on the augmented data explained in Section \ref{sec:dawiki}.
After pretraining for 50 epochs with the same hyperparameters as the main experiment, the following performance was observed.

\begin{table}[H]
    \centering

    \begin{tabular}{l|l|cccccc}
        Model                                 & Top $k$-accuracy [\pro]  & $k=1$  & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$\\\hline
        \multirow{2}{*}{Limited entity vocab.}& Masked words             & 23.91  & 30.50 & 33.90 & 39.19  & 47.11  & 53.03 \\
                                              & Masked entities          & 30.56  & 42.15 & 48.87 & 55.79  & 65.91  & 66.32
    \end{tabular}
    \caption{
        The top $k$ accuracy of the entity vocabulary-limited model in the 50'th and last epoch.
    }
    \label{tab:few-ents-acc}
\end{table}\noindent
The performance on the word prediction task is slightly worse for this entity vocabulary experiment, while clearly higher scores are seen on the masked entity task.
As for the data augmentation experiment, attention must again be put on the changes in the benchmark itself as the accuracy of 31\pro in calculated in a $\sim 20\ctp3$ class problem while the baseline accuracy of 29\pro is calculated over almost ten times as many classes.

% 2021-06-11 19:38:47.989    INFO        K=1
%                                        Word:   23.905
%                                        Entity: 30.585
% 2021-06-11 19:38:47.998    INFO        K=3
%                                        Word:   30.500
%                                        Entity: 42.149
% 2021-06-11 19:38:48.005    INFO        K=5
%                                        Word:   33.902
%                                        Entity: 47.872
% 2021-06-11 19:38:48.011    INFO        K=10
%                                        Word:   39.185
%                                        Entity: 55.786
% 2021-06-11 19:38:48.017    INFO        K=25
%                                        Word:   47.108
%                                        Entity: 65.914
% 2021-06-11 19:38:48.022    INFO        K=50
%                                        Word:   53.028
%                                        Entity: 73.347

The NER performance of this model was, following the approach in the previous experiments, found to be quite close to the performance of the baseline with an average F1 0.2\pro\ points worse.

\begin{table}[H]
    \centering
    \begin{tabular}{l|ccccc|c|c}
        \multirow{2}{*}{Model}  & \multicolumn{5}{c|}{F1 [\pro]} & Precision [\pro]               & Recall [\pro]               \\
                            & Avg. & LOC & PER & ORG & MISC      & Avg.                           & Avg.                         \\ \hline
    Limited entity vocab.   & 82.22&89.69&93.59&74.00&68.72      & 85.02                          & 79.57
    \end{tabular}
    \caption{The finetuning results of the entity vocabulary limiting pretraining experiment.}
    \label{tab:dataaug}
\end{table}
% 2021-06-11 15:26:30.316    INFO                      precision    recall  f1-score   support

%                                                 LOC     0.8878    0.9062    0.8969        96
%                                                MISC     0.7358    0.6446    0.6872       121
%                                                 ORG     0.7986    0.6894    0.7400       161
%                                                 PER     0.9385    0.9333    0.9359       180

%                                           micro avg     0.8506    0.7957    0.8222       558
%                                           macro avg     0.8402    0.7934    0.8150       558
%                                        weighted avg     0.8455    0.7957    0.8188       558

Conclusively, the results of this experiment are close to the baseline, signifying robustness in the approach.
However, with these hyperparameters, the removal of entities does not seem promising for the Danish data set.

This experimental filtering of the DaLUKE entities preserves $7\pro$ of the content pages, while $8\pro$ were preserved for the English LUKE.
Intuitively, it would make sense that the same proportional filtering works better in English, as the absolute number of entities making up the implicit knowledge base of the model might be important:
Smaller datasets, corresponding to smaller language areas, do not equate a smaller shared world of named entities.

This conceptual argument somewhat supported by the experimental results suggests that more of the rare entities should be included when working with smaller datasets, though a more continuous experimental search of this filtering parameter might yield a better trade-off than our main model solution of including all entities.

\subsection{Summary} \label{subsec:preoverview}

\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabular}{l|l|cccccc}
        Model                                 & Top $k$-accuracy [\pro]  & $k=1$  & $k=3$ & $k=5$ & $k=10$ & $k=25$ & $k=50$\\\hline
        \multirow{2}{*}{Baseline}             & Masked words             & 24.23  & 30.94 & 34.37 & 39.70  & 47.71  & 53.62 \\
                                              & Masked entities          & 28.80  & 39.01 & 50.64 & 50.64  & 59.57  & 66.32 \\\hline
        \multirow{2}{*}{No data aug.}         & Masked words             & \underline{25.26}  & \underline{32.16} & \underline{35.60} & \underline{40.93}  & \underline{48.78}  & \underline{54.62} \\
                                              & Masked entities          & \underline{34.02}  & 42.85 & 47.21 & 53.52  & 62.26 & \underline{68.82} \\\hline
        \multirow{2}{*}{BERT attention}       & Masked words             & 20.63  & 26.29 & 29.41 & 34.58  & 42.44  & 48.73 \\
                                              & Masked entities          & 12.26  & 20.98 & 25.69 & 32.76  & 42.88 & 51.52 \\\hline
        \multirow{2}{*}{No transfer learning} & Masked words             & \bfseries 47.86  & \bfseries 59.28 & \bfseries 63.92 & \bfseries 69.83  & \bfseries 76.51  & \bfseries 81.04 \\
                                              & Masked entities          & \bfseries 81.71  & \bfseries 87.19 & \bfseries 88.87 & \bfseries 90.90  & \bfseries 93.15 & \bfseries 94.56 \\\hline
        \multirow{2}{*}{Limited entity vocab.}& Masked words             & 23.91  & 30.50 & 33.90 & 39.19  & 47.11  & 53.03 \\
                                              & Masked entities          & 30.56  & \underline{42.15} & \underline{48.87} & \underline{55.79}  & \underline{65.91}  & 66.32
    \end{tabular}
    \caption{
        Overview of final pretraining results for all the experiments presented previously.
        Best result for each metric is shown in boldface with second best result underlined (as the distribution of bests is quite boring here).
    }
    \label{tab:mlmsummary}
\end{table}

\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabular}{l|ccccc|c|c}
        \multirow{2}{*}{Model}  & \multicolumn{5}{c|}{F1 [\pro]} & Precision [\pro]               & Recall [\pro]               \\
                            & Avg. & LOC & PER & ORG & MISC      & Avg.                           & Avg.                        \\ \hline
    Baseline                & 82.41&89.58&92.90&74.48&68.84      & \textbf{86.73}                          & 78.49                       \\
    No data augmentation    & \textbf{83.07}&84.42&\textbf{95.18}&\textbf{76.41}&\textbf{72.10}      & 85.42                          & \textbf{80.82}                       \\
    BERT attention          & 79.62 & 85.86 & 92.84 & 70.95 &   64.84      & 84.51                          & 75.27 \\
    No transfer learning    & 71.04&79.05&83.57&55.42&61.74      & 76.99                          & 65.95                       \\
    Limited entity vocab.   & 82.22&\textbf{89.69}&93.59&74.00&68.72      & 85.02                          & 79.57 
    \end{tabular}
    \caption{Overview of the pretraining experiment finetuning results presented in the previous subsections.}
    \label{tab:nersummary}
\end{table}

\section{Finetuning Performance}
\label{sec:finetuning-exp}
\begin{enumerate}
    \item Prec. vs rec. - bør måske være i 6.3, da det jo er mere analyse-agtigt?
%    \item Gør det klart, at alle bruger den store Carlos
    \item Gør det klart, hvad baseline er
    
\end{enumerate}
As with pretraining, fine-tuning has a number of interesting elements that are worth investigating further.
For these experiments, the main pretrained model from section \ref{sec:Pretraining of daLUKE} are used but with the baseline fine-tuning hyperparameters from table \ref{tab:baseline-hyper}.
Following the pretraining experiments, all results are reported with MISC.

\subsection{Feature Usage}%
\label{sub:Feature Usage}
An entity candidate forward passed through the final, classifying layer in the NER model is comprised of the concatenation of word representations of first and final sub-word tokens, and, novelly for LUKE, a contextual representation of the entity span.
Two finetuning experiments are performed by altering this entity feature approach by observing the performance when the sub-word token and entity representations are used individually.

\begin{enumerate}
    \item Konkaterneringsshow
\end{enumerate}

\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabular}{l|ccccc|c|c}
        \multirow{2}{*}{Model}  & \multicolumn{5}{c|}{F1 [\pro]} & Precision [\pro]               & Recall [\pro]               \\
        & Avg. & LOC & PER & ORG & MISC      & Avg.                           & Avg.                        \\ \hline
        First and last sub-word tokens & 80.93 & 85.57 & 93.00 & 70.47 & 71.86 & 83.72 & 78.32 \\
        Entity tokens & 80.30 & 83.24 & 92.39 & 72.43 & 69.33 & 84.25 & 76.70 \\
        Both (default) & & & & & & & 
    \end{tabular}
    \caption{Fine-tuning results using different features for the classifier.}
    \label{tab:concat}
\end{table}


\subsection{Pretraining Impact}
The pretraining effect is difficult to quantify with questions such as validity of pretraining task for language understanding and overfitting on the limited data set being scary.
One way to glimpse into the pretraining black box is to observe the performance of down stream language tasks on a number of checkpoints produced during the pretraining.
Finetuning of the main DaLUKE after every fifth epoch was performed using the finetuning experiment hyperparameters seen at \ref{tab:baseline-hyper}.

\begin{enumerate}
    \item Fine-tuning på en række checkpoints
\end{enumerate}

% \subsection{Class-weighted Loss} %TODO: Interessant

\subsection{Stability of LUKE Finetuning}

\subsubsection{Reproducability of English LUKE}
For the reproduction of the English LUKE, the results were initially underwhelming as five repetitions of the finetuning of LUKE large on CoNLL2003 consistently, slightly underperformed the reported result of 94.3\pro\ micro average as seen at table \ref{tab:EnLUKE-wrong}.
After investigating, this somewhat disappointing result was revealed to be overturned when finetuning with Python version 3.6 and PyTorch 1.2 instead of using newer versions as done in the initial run.
Using these correct versions, the same as Yamada et al. reported in their package file, the results were succesfully reproduced as shown in the central results in Section \ref{sec:English LUKE Reproduction}.
For the large model, only one of the five repetitions yielded the reported performance.
No criticism can be raised based on this results; the sample size is small and, furthermore, we found that the mean \emph{base} model performance superseded the reported micro average F1 score of 93.3\pro.

All in all, the slight hiccups in reproducing the finetuning results show the difficulties in empiric evaluations of deep methods strongly relying on indeterminism -- especially as the underlying framework, PyTorch, does not guarantee reproducable results across versions and platforms, even if using identical seeds for the random number generator \cite{pytorchrep}.

\begin{table}[H]
    \begin{center}
            \begin{tabular}{l r r r r r}
                    Model & Micro avg. & LOC & PER & ORG & MISC \\
                    \hline
                    LUKE large & $93.97 \pm  0.07$ & $95.06 \pm  0.2$ & $97.19 \pm  0.08$ & $93.51 \pm  0.2$ & $85.15 \pm  0.4$ \\
                        &  &  &  &  &  \\
                    Support & 5616 & 1666 & 1602 & 1647 & 701 \\
            \end{tabular}
    \end{center}
    \caption{
        Results when pre-training and evaluating LUKE with Python version 3.8 and PyTorch 1.4.
        Values are mean and standard deviance of F1 scores over five repetitions of the fine-tuning.
    }
    \label{tab:EnLUKE-wrong}
\end{table}

\subsubsection{Randomness of DaLUKE Results}
The same repetition of the finetuning procedure as carried out for English LUKE is done for the main DaLUKE pretrained model by training with five different seeds.
\begin{table}[H]
    \centering
    \begin{tabular}{l|rrrrr|r|r}
    \multirow{2}{*}{Seed}  & \multicolumn{5}{c|}{F1}        & \multirow{2}{*}{Avg. Precision} & \multirow{2}{*}{Avg. Recall}\\
                            & Avg. & LOC & PER & ORG & MISC  &                                &                             \\ \hline
     1                      &      &     &     &     &       &                                &                             \\
     2                      &      &     &     &     &       &                                &                             \\
     3                      &      &     &     &     &       &                                &                             \\
     4                      &      &     &     &     &       &                                &                             \\
     5                      &      &     &     &     &       &                                &
    \end{tabular}
    \caption{Repeating the finetuning procedure of the main model, reporting micro averages.}
    \label{tab:seeds}
\end{table}


\begin{enumerate}
    \item Gentagelse af finetuning for fem forskellige seeds
    \item Krydsval. af finetuning
    \item Statistiske metoder til sammenligning af scores
    \item Spørgsmålstegn ved generaliserbarhed
\end{enumerate}

\section{Predictions: What Is Learned?}

\subsection{Masked Language Predictions}
\begin{enumerate}
    \item Noget analyse og nogle eksempler på MLM, MEM-forudsigelser
\end{enumerate}

\subsection{daLUKE Representation Geometry}

\begin{enumerate}
    \item Visualiseringer af DaNE-repræsentationeres geometri (PCA, t-SNE, UMAP)
    \item Forsøg på forklaring af resultaterne
\end{enumerate}

\subsection{When the Model is Wrong}

\begin{enumerate}
    \item Fejl-analyse på modellen: Print eksempler på fejl og sammenlign gerne med, hvilke fejl konkurrenterne begår
    \item Forvirringsmatricer
\end{enumerate}

\end{document}
