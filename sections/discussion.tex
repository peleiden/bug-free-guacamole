\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Further Experiments and Discussion}
\lhead{Further Experiments and Discussion}

\section{LUKE's predictions}
\subsection{Reproducability}
\begin{table}[H]
    \begin{center}
            \begin{tabular}{l r r r r r}
                    Model & Micro avg. & LOC & PER & ORG & MISC \\
                    \hline
                    LUKE large & $93.97 \pm  0.07$ & $95.06 \pm  0.2$ & $97.19 \pm  0.08$ & $93.51 \pm  0.2$ & $85.15 \pm  0.4$ \\
                        &  &  &  &  &  \\
                    Support & 5616 & 1666 & 1602 & 1647 & 701 \\
            \end{tabular}
    \end{center}
    \caption{
        Results when pre-training and evaluating LUKE with Python version 3.8 and PyTorch 1.4.
        Values are mean and standard deviance over five repetitions of the fine-tuning.
    }
\end{table}

\section{Pretraining Experiments}
In order to investigate what affects pretraining effectiveness, several ablation studies are performed.
Unless stated otherwise, the hyperparameters from table \ref{tab:pretrain-hyper} are used, but with 50 epochs due to limited compute.

\paragraph{Baseline}
For the baseline model, the main model is retrained using only 50 epochs.
This allows for a fairer comparison to the ablation studies as apposed to using the model trained for 150 epochs.


\paragraph{Hyperparameter Search}

\paragraph{Dataset Augmentation}

\paragraph{Bert Attention}

\paragraph{Fixing of da-BERT Parameters}

\paragraph{Weighted Entity Loss}

\section{Finetuning}
\label{sec:finetuning}
% Starter med at lære "O" på alting, derefter kommer detaljerede ting
% Undersøg varians på læring - særligt fokus på lr, seed og måske dropout

\end{document}
