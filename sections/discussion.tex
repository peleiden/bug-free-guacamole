\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Experiments and Discussion}
\lhead{Experiments and Discussion}

\section{What Is Going on in the Pretraining?}
In order to investigate what affects pretraining effectiveness, several ablation studies are performed.
Unless stated otherwise, the hyperparameters from table \ref{tab:pretrain-hyper} are used, but with 50 epochs due to limited compute.

\subsection{Introspection: The Parameter Population}

\subsection{Baseline}
For the baseline model, the main model is retrained using only 50 epochs.
This allows for a fairer comparison to the ablation studies as apposed to using the model trained for 150 epochs.

\subsection{Dataset Augmentation}
A key addition to the pretraining pipeline was the addition of extra entity annotations not already in the Wikipedia articles themselves using pattern matching as explained in Section~\ref{subsec:entaug}.
This resulted in a 48\pro growth in the number of annotations as shown in Table~\ref{tab:metadata}.
An ablation experiment on this was performed

\ref{tab:metadata}
\begin{enumerate}
    \item Performance og træningskurver uden datasæt-augmentering
\end{enumerate}

The theorized problem of of false negatives in the volunteer-produced annotations seems to be present in the data and this augmentation appears to mitigate this.
This result supports a possible strategy for improvement:
To produce additional LUKE pretraining data sets from raw text corpora by automatically annotating them using pattern matching.
Such lower standard annotations used to achieve extra data might be necessary for continued improvement in low-resource languages such as Danish.

\subsection{Entity-Aware Attention}
\begin{enumerate}
    \item Performance og træningskurver uden entity-aware attention
    \item Køretidssammenligning på de to modeller
\end{enumerate}

\subsection{Impact of Danish BERT}
\begin{enumerate}
    \item Performance og træningskurver for eksperimentet, hvor BERT-vægte ikke bruges
    \item Her rapporteres under-eksperimentet med performance for da-BERT på vores datasæt
    \item Her kan henvises til parameterplotsene i en diskussion af BERT-vægtenes rolle
\end{enumerate}

\subsection{Entity Vocabulary Size}
For the English LUKE, Yamada et al. used an entity vocabulary of the $500\ctp3$ entities \cite[Sec. 3.4]{yamada2020luke} even though the English Wikipedia contains $\sim 6\ctp6$ content pages\footnotemark.
\footnotetext{English Wikipedia Statistics: \url{https://en.wikipedia.org/wiki/Special:Statistics}}
This is, apart from crude filtering of non-entity pages, a result of cutting the entity domain down to the most frequent hyperlinks.
For the Danish Wikipedia, however, the number of content pages is $\sim 250\ctp3$ \footnotemark resulting in the DaLUKE entity vocabulary containing $220\ctp3$ entities.
\footnotetext{Danish Wikipedia Statistics: \url{https://da.wikipedia.org/wiki/Speciel:Statistik}}
As this is a much smaller world of entities, the main DaLUKE model did not cut this full entity vocabulary by frequency.

An experiment was performed where entities that were mentioned less than 50 times were excluded from the data, resulting in a vocabulary of $21,769$ entities.
This was done on the augmented data explained in Section \ref{sec:dawiki}.

\begin{enumerate}
    \item Performance og træningskurver
\end{enumerate}

This experimental filtering of the DaLUKE entities preserves $7,8\pro$ of the content pages, while $8,3\pro$ were preserved for the English LUKE.
Intuitively, it would make sense that the same proportional filtering works better in English, as the absolute number of entities making up the implicit Knowledge Base of the model might be important:
Smaller datasets, corresponding to smaller language areas, does not equate a smaller shared world of named entities.

This conceptual argument together with the experimental results suggests that more of the rare entities should be included when working with smaller datasets, though a more continuous experimental search of this filtering parameter might yield a better trade-off than our solution of including all entities.

\section{Finetuning Performance}
\begin{enumerate}
    \item Prec. vs rec
\end{enumerate}

\subsection{Feature Usage}%
\label{sub:Feature Usage}

\begin{enumerate}
    \item Konkaterneringsshow
\end{enumerate}


\subsection{Pretraining Impact}
\begin{enumerate}
    \item Fine-tuning på en række checkpoints
\end{enumerate}

\subsection{Hyperparameter Impact}

\begin{enumerate}
    \item Hyperparametersøgning for daluke-ner
\end{enumerate}

\subsection{Stability of Finetuning}

\subsubsection{Reproducability of English LUKE}
\begin{table}[H]
    \begin{center}
            \begin{tabular}{l r r r r r}
                    Model & Micro avg. & LOC & PER & ORG & MISC \\
                    \hline
                    LUKE large & $93.97 \pm  0.07$ & $95.06 \pm  0.2$ & $97.19 \pm  0.08$ & $93.51 \pm  0.2$ & $85.15 \pm  0.4$ \\
                        &  &  &  &  &  \\
                    Support & 5616 & 1666 & 1602 & 1647 & 701 \\
            \end{tabular}
    \end{center}
    \caption{
        Results when pre-training and evaluating LUKE with Python version 3.8 and PyTorch 1.4.
        Values are mean and standard deviance over five repetitions of the fine-tuning.
    }
\end{table}

\subsubsection{Randomness of Results}
\begin{enumerate}
    \item Krydsval. af finetuning
    \item Gentagelse af finetuning for fem forskellige seeds
    \item Statistiske metoder til sammenligning af scores
    \item Spørgsmålstegn ved generaliserbarhed
\end{enumerate}

\section{Predictions: What Is Learned?}

\subsection{Masked Language Predictions}
\begin{enumerate}
    \item Noget analyse og nogle eksempler på MLM, MEM-forudsigelser
\end{enumerate}

\subsection{daLUKE Representation Geometry}

\begin{enumerate}
    \item Visualiseringer af DaNE-repræsentationeres geometri (PCA, t-SNE, UMAP)
    \item Forsøg på forklaring af resultaterne
\end{enumerate}

\subsection{When the Model is Wrong}

\begin{enumerate}
    \item Fejl-analyse på modellen: Print eksempler på fejl og sammenlign gerne med, hvilke fejl konkurrenterne begår
\end{enumerate}

\end{document}
