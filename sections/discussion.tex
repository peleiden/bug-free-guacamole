\documentclass[main.tex]{subfiles}

\begin{document}
\chapter{Experiments and Discussion}
\lhead{Experiments and Discussion}

\section{What Is Going on in the Pretraining?}
In order to investigate what affects pretraining effectiveness, several ablation studies are performed.
Unless stated otherwise, the hyperparameters from table \ref{tab:pretrain-hyper} are used, but with 50 epochs due to limited compute.

\subsection{Introspection: The Parameter Population}

\subsection{Baseline}
For the baseline model, the main model is retrained using only 50 epochs.
This allows for a fairer comparison to the ablation studies as apposed to using the model trained for 150 epochs.

\subsection{Dataset Augmentation}
\begin{enumerate}
    \item Performance og træningskurver uden datasæt-augmentering
\end{enumerate}

\subsection{Entity-Aware Attention}
\begin{enumerate}
    \item Performance og træningskurver uden entity-aware attention
    \item Køretidssammenligning på de to modeller
\end{enumerate}

\subsection{Impact of Danish BERT}
\begin{enumerate}
    \item Performance og træningskurver for eksperimentet, hvor BERT-vægte ikke bruges
    \item Her rapporteres under-eksperimentet med performance for da-BERT på vores datasæt
    \item Her kan henvises til parameterplotsene i en diskussion af BERT-vægtenes rolle
\end{enumerate}

\subsection{Entity Vocabulary Size}

\begin{enumerate}
    \item Performance og træningskurver for eksperimentet, hvor vocab'en blev begrænset - er dette spændende nok?
\end{enumerate}

\section{Finetuning Performance}
\begin{enumerate}
    \item Prec. vs rec
\end{enumerate}

\subsection{Featre Usage}%
\label{sub:Featre Usage}

\begin{enumerate}
    \item Konkaterneringsshow
\end{enumerate}


\subsection{Pretraining Impact}
\begin{enumerate}
    \item Fine-tuning på en række checkpoints
\end{enumerate}

\subsection{Hyperparameter Impact}

\begin{enumerate}
    \item Hyperparametersøgning for daluke-ner
\end{enumerate}

\subsection{Stability of Finetuning}

\subsubsection{Reproducability of English LUKE}
\begin{table}[H]
    \begin{center}
            \begin{tabular}{l r r r r r}
                    Model & Micro avg. & LOC & PER & ORG & MISC \\
                    \hline
                    LUKE large & $93.97 \pm  0.07$ & $95.06 \pm  0.2$ & $97.19 \pm  0.08$ & $93.51 \pm  0.2$ & $85.15 \pm  0.4$ \\
                        &  &  &  &  &  \\
                    Support & 5616 & 1666 & 1602 & 1647 & 701 \\
            \end{tabular}
    \end{center}
    \caption{
        Results when pre-training and evaluating LUKE with Python version 3.8 and PyTorch 1.4.
        Values are mean and standard deviance over five repetitions of the fine-tuning.
    }
\end{table}

\subsubsection{Randomness of Results}
\begin{enumerate}
    \item Krydsval. af finetuning
    \item Gentagelse af finetuning for fem forskellige seeds
    \item Statistiske metoder til sammenligning af scores
    \item Spørgsmålstegn ved generaliserbarhed
\end{enumerate}

\section{Predictions: What Is Learned?}

\subsection{Masked Language Predictions}
\begin{enumerate}
    \item Noget analyse og nogle eksempler på MLM, MEM-forudsigelser
\end{enumerate}

\subsection{daLUKE Representation Geometry}

\begin{enumerate}
    \item Visualiseringer af DaNE-repræsentationeres geometri (PCA, t-SNE, UMAP)
    \item Forsøg på forklaring af resultaterne
\end{enumerate}

\subsection{When the Model is Wrong}

\begin{enumerate}
    \item Fejl-analyse på modellen: Print eksempler på fejl og sammenlign gerne med, hvilke fejl konkurrenterne begår
\end{enumerate}

\end{document}
