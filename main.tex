% !TeX document-id = {b85b6db0-ec5f-47d1-b86b-328dfa82bb16}
% !BIB TS-program = biber
\documentclass[12pt, fleqn]{report}

\input{praeambulum.tex}

\addbibresource{references.bib}

\chead{}
\rhead{Technical University of Denmark}
\rfoot{Page \thepage{}~of \pageref{LastPage}}

\graphicspath{{imgs/}{../imgs/}}
\linespread{1.15}

\begin{document}

%https://en.wikibooks.org/wiki/LaTeX/Title_Creation
\begin{titlepage}
    \centering
    {\scshape\LARGE Technical University of Denmark \par}
    \vspace{1cm}
    {\scshape\Large Bachelor project in Artificial Intelligence and Data\par}
    \vspace{1.5cm}
    {\huge\bfseries DaLUKE: The Entity-aware, Danish Language Model\par}
    \vspace{2cm}
    \begin{large}
        \centering
        \begin{tabular}{ccc}
            Søren Winkel Holm & Asger Laurits Schultz\\
            \code{s183911@dtu.dk} & \code{s183912@dtu.dk}
        \end{tabular}
    \end{large}\par
    \vfill
    Project: \textit{An Open Source Danish Knowledge Graph Language Model}\par
    Software repository:
    \code{\href{https://github.com/peleiden/daLUKE}{github.com/peleiden/daluke}}\par
    % Demonstration:
    % \code{\href{https://peleiden.github.io/daLUKE}{peleiden.github.io/daLUKE}}\par
    \vspace{2cm}
    Supervisors:\par
    Lars Kai Hansen, DTU Compute\\
    Michael Riis Andersen, DTU Compute\\
    Victor Elkjær Birk, IBM Services
    \vfill
    {\large \today\par}
\end{titlepage}
\begin{abstract}
    The advent of deep learning has led to significant advances in the field of natural language processing in recent years, but many models, while good at modelling natural language lack explicit knowledge, making entity related tasks challenging.
    LUKE, proposed by Yamada et al. \cite{yamada2020luke} in October, 2020, is a transformer \cite{vaswani2017att} based architecture that explicitly models entities, allowing it to achieve state of the art on several entity related benchmarks, including named entity recognition (NER).
    In this report, LUKE's NER results are reproduced along with leading Danish NER results, and an open source Danish LUKE, DaLUKE, is produced and released in two versions:
    Firstly, a pretrained model for producing contextualized word and entity representations.
    Secondly, a fine-tuned model trained on the leading Danish NER dataset, DaNE \cite{hvingelby2020dane}, on which it achieves close to state of the art, beating BotXO's Danish BERT \cite{botxo2019dabert}.
    Several ablation studies are conducted to explore what effects different techniques have on performance.
    Finally, an open source software package, \code{daluke}, is released with the goal of making DaLUKE easy to use.
\end{abstract}

% \setcounter{tocdepth}{1}
\tableofcontents
% \setcounter{tocdepth}{2}

\setlength{\headheight}{15pt}
\addtolength{\topmargin}{-2.5pt}.

\subfile{sections/intro}

\subfile{sections/theory.tex}

\subfile{sections/data}

\subfile{sections/methods}

\subfile{sections/results}

\subfile{sections/discussion}

\subfile{sections/conclusion}

% Smaller bib text
\renewcommand*{\bibfont}{\normalfont\footnotesize}.
\printbibliography[heading=bibintoc]

\appendix
\subfile{sections/appendix}

\end{document}
