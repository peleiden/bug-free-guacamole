% !TeX spellcheck = en_GB
% !TeX document-id = {b85b6db0-ec5f-47d1-b86b-328dfa82bb16}
% !BIB TS-program = biber
\documentclass[12pt, fleqn]{report}

\input{praeambulum.tex}

\addbibresource{references.bib}

\chead{}
\rhead{Technical University of Denmark}
\rfoot{Page \thepage{}~of \pageref{LastPage}}

\graphicspath{{imgs/}{../imgs/}}
\linespread{1.15}
\newcommand{\abbrv}[2]{\vspace{0.1cm}\textbf{#1} & #2\\}

\begin{document}

%https://en.wikibooks.org/wiki/LaTeX/Title_Creation
\begin{titlepage}
    \centering
    {\huge\bfseries DaLUKE:\\The Knowledge-enhanced, Danish Language Model\par}
    \vspace{1.5cm}
    \includegraphics[width=0.2\linewidth]{dtu-logo}\\[4ex]
    \vspace{.5cm}
    {\scshape\LARGE Technical University of Denmark \par}
    \vspace{.5cm}
    {\scshape\large Bachelor's Project in Artificial Intelligence and Data\par}
    \vspace{2cm}
    \begin{large}
        \centering
        \begin{tabular}{ccc}
            Søren Winkel Holm & Asger Laurits Schultz\\
            \code{s183911@dtu.dk} & \code{s183912@dtu.dk}
        \end{tabular}
    \end{large}\par
    \vfill
    % Demonstration:
    % \code{\href{https://peleiden.github.io/daLUKE}{peleiden.github.io/daLUKE}}\par
    Supervisors:\par
    Lars Kai Hansen, DTU Compute\\
    Michael Riis Andersen, DTU Compute\\
    Victor Elkjær Birk, IBM Services
    \vfill
    {\large \today\par}
\end{titlepage}
\begin{abstract}
    The advent of deep learning has led to significant advances in the field of natural language processing in recent years, but many models, while good at modeling language, lack explicit knowledge, making tasks involving real-world, factual entities challenging.
    LUKE, proposed by Yamada et al. in October 2020, is a transformer-based architecture that explicitly models entities, allowing it to achieve state of the art on several benchmarks, including named entity recognition (NER).
    In this report, LUKE's English NER results are reproduced along with leading Danish NER results, and an open source Danish LUKE, DaLUKE, is produced.
    Firstly, a general, pretrained model for producing contextualized word and entity representations is released.
    Secondly, a model is presented which is trained on the central Danish NER dataset, DaNE, achieving close to state of the art and slightly outperforming BotXO's Danish BERT, though within margin or error.
    Several ablation studies are conducted to explore what effects different techniques have on performance.
    Finally, an open source software package, \code{daluke}, is released with the goal of making knowledge-based deep learning for Danish easy to use.
\end{abstract}

\chapter*{Preface}
This project has been completed to fulfil the requirements to obtain the bachelor's degree in Artificial Intelligence and Data at the Technical University of Denmark.
Per author, the project was assigned a workload of 15 ECTS corresponding to approximately 420 working hours.
The software presented in the project is available at \code{\href{https://github.com/peleiden/daLUKE}{github.com/peleiden/daluke}}.

We are incredibly grateful for the large amount of help we received from DTU and broadly from Danish natural language processing practitioners.
Obligatory mentions include Rasmus Arpe and Martin Nielsen from Danspeech, Finn Årup Nielsen from DTU Compute, and Kasper Lindskou and Lukas Nielsen from Ekstra Bladet Analyse for valuable advice, especially on data concerns.
We are also grateful for the help of Ikuya Yamada, Studio Ousia for publishing LUKE and openly answering all our questions on modeling details.

A special thank you goes out to Johannes Kruse for continuous feedback and insight.
All of our three supervisors gave us invaluable guidance and encouragement crucial for the project.
We would never have learned so much about this exciting field without the ambition and motivation of Lars Kai Hansen.

Finally, we note that this project presents text examples in Danish that are not translated to English.
A report meant for a broader audience would require such translations.
% \setcounter{tocdepth}{1}

\chapter*{Abbreviations}
\begin{table}[H]
    \begin{tabular}{@{}l l}
        \abbrv{AI}{Artificial intelligence}
        \abbrv{AMP}{Automatic mixed precision}
        \abbrv{BPE}{Byte-pair encoding}
        \abbrv{CER}{Contextualized entity representation}
        \abbrv{CRF}{Conditional random field}
        \abbrv{CWR}{Contextualized word representation}
        \abbrv{FN}{False negative}
        \abbrv{FP}{False positive}
        \abbrv{GLUE}{General Language Understanding Evaluation}
        \abbrv{IOB}{Inside-outside-beginning}
        \abbrv{KB}{Knowledge base}
        \abbrv{KG}{Knowledge graph}
        \abbrv{LM}{Language model}
        \abbrv{MLM}{Masked language model}
        \abbrv{NE}{Named entity}
        \abbrv{NER}{Named entity recognition}
        \abbrv{NLP}{Natural language processing}
        \abbrv{PCA}{Principal component analysis}
        \abbrv{RNG}{Random number generation}
        \abbrv{SB}{Sub-batch size}
        \abbrv{SOTA}{State of the art}
        \abbrv{STLR}{Slanted triangle learning rate}
        \abbrv{TN}{True negative}
        \abbrv{TP}{True positive}
        \abbrv{UD-DDT}{Universal Dependencies of Danish Dependency Treebank}
    \end{tabular}
\end{table}\noindent

\tableofcontents
% \setcounter{tocdepth}{2}

\setlength{\headheight}{15pt}
\addtolength{\topmargin}{-2.5pt}.

\subfile{sections/intro}

\subfile{sections/theory}

\subfile{sections/data}

\subfile{sections/methods}

\subfile{sections/results}

\subfile{sections/discussion}

\subfile{sections/conclusion}

% Smaller bib text
\renewcommand*{\bibfont}{\normalfont\footnotesize}
\lhead{Bibliography}
\printbibliography[heading=bibintoc]

\appendix
\subfile{sections/appendix}

\end{document}
